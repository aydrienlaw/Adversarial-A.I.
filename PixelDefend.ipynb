{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256084fa",
   "metadata": {},
   "source": [
    "**This notebook focuses on the effectiveness of PixelDefend against adversarial attacks on the MNIST and MARVEL datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdf42",
   "metadata": {},
   "source": [
    "## **Section 0 - Setting Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbf7b5",
   "metadata": {},
   "source": [
    "### **Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d608e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, Layer\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from art import config\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, ProjectedGradientDescent, SaliencyMapMethod, CarliniL2Method, NewtonFool, BasicIterativeMethod\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier, PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2e0d4",
   "metadata": {},
   "source": [
    "### Load PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0f0b",
   "metadata": {},
   "source": [
    "### **Modification: Disabling eager execution to enable adversarial crafting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87648805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8b7ec",
   "metadata": {},
   "source": [
    "### **Load MARVEL dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test_cln = []\n",
    "y_test_cln = [] \n",
    "min_pixel_value = 0\n",
    "max_pixel_value = 1\n",
    "\n",
    "def marvel_class(filename):\n",
    "    switcher={\n",
    "        'HeavyLoadCarrier': [1,0,0,0,0,0,0,0,0],\n",
    "        'CombatVessel': [0,1,0,0,0,0,0,0,0],\n",
    "        'ContainerShip': [0,0,1,0,0,0,0,0,0],\n",
    "        'PassengersShip': [0,0,0,1,0,0,0,0,0],\n",
    "        'Ro-roCargo': [0,0,0,0,1,0,0,0,0],\n",
    "        'Tanker': [0,0,0,0,0,1,0,0,0],\n",
    "        'Tug': [0,0,0,0,0,0,1,0,0],\n",
    "        'SupplyVessel': [0,0,0,0,0,0,0,1,0],\n",
    "        'Yacht': [0,0,0,0,0,0,0,0,1]\n",
    "    }\n",
    "    return switcher.get(filename)\n",
    "\n",
    "def load_training_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/train_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:      \n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_train_cln.append(img/255)\n",
    "            y_train_cln.append(marvel_class(filename))\n",
    "            i = i+1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_train_cln, y_train_cln\n",
    "\n",
    "def load_test_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/test_9/\"+filename\n",
    "    i = 0\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_test_cln.append(img/255)\n",
    "            y_test_cln.append(marvel_class(filename))\n",
    "            i = i + 1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_test_cln, y_test_cln\n",
    "\n",
    "# for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/train_9\"):\n",
    "#     load_training_data(filename)\n",
    "#     print(filename)\n",
    "\n",
    "for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"):\n",
    "    load_test_data(filename)\n",
    "    print(filename)\n",
    "    \n",
    "#load_training_data(\"/home/cyber/Desktop/Adrian/marvel_data/test_9/CombatVessel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6df68",
   "metadata": {},
   "source": [
    "*Modification: Convert MARVEL x_test/x_train from uint8 into float32, to enable classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln = np.array(x_test_cln, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58648ad",
   "metadata": {},
   "source": [
    "### **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_cln, y_train_cln), (x_test_cln, y_test_cln), min_pixel_value, max_pixel_value = load_mnist()\n",
    "# x_train_cln = x_train_cln.astype(np.float32)\n",
    "# x_test_cln, y_test_cln = x_test_cln[:1000], y_test_cln[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c4a58",
   "metadata": {},
   "source": [
    "### **Load / Create classifier model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c095be",
   "metadata": {},
   "source": [
    "*MNIST pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"/home/cyber/mnist_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa596aeb",
   "metadata": {},
   "source": [
    "*MARVEL pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab83ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/cyber/Desktop/Adrian/Xception-10-0.74.hdf5\"\n",
    "model = load_model(model_path, custom_objects={'RAdam': RAdam}, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f6bd9",
   "metadata": {},
   "source": [
    "*Optional step: Train and save a model for future use*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_cln, y_train_cln, batch_size=64, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ff0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/cyber/dataset_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2df92",
   "metadata": {},
   "source": [
    "*Create ART classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc077ab",
   "metadata": {},
   "source": [
    "## **Section 1 - Attack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9dd4",
   "metadata": {},
   "source": [
    "Step 1: Evaluate the classifier on clean test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln = classifier.predict(x_test_cln)\n",
    "accuracy_cln = np.sum(np.argmax(predictions_cln, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy_cln * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac38ca",
   "metadata": {},
   "source": [
    "Step 2: Split clean test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_cln_indexes=[]\n",
    "fp_cln_indexes=[]\n",
    "x_test_cln_tp=[]\n",
    "y_test_cln_tp=[]\n",
    "x_test_cln_fp=[]\n",
    "y_test_cln_fp=[]\n",
    "\n",
    "for k in range(len(predictions_cln)):\n",
    "    if(np.argmax(predictions_cln, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_cln_indexes.append(k)\n",
    "    else:\n",
    "        fp_cln_indexes.append(k)\n",
    "\n",
    "for k in tp_cln_indexes:\n",
    "    x_test_cln_tp.append(x_test_cln[k])\n",
    "    y_test_cln_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_cln_indexes:\n",
    "    x_test_cln_fp.append(x_test_cln[k])\n",
    "    y_test_cln_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_cln_tp = np.array(x_test_cln_tp)\n",
    "x_test_cln_fp = np.array(x_test_cln_fp)\n",
    "\n",
    "print('Number of benign true positives: {:}'.format(len(x_test_cln_tp)))\n",
    "print('Number of benign false positives: {:}'.format(len(x_test_cln_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b91dd",
   "metadata": {},
   "source": [
    "Step 3: Craft adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e07528",
   "metadata": {},
   "source": [
    "*Jacobian-based Saliency Map Attack (JSMA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = SaliencyMapMethod(classifier=classifier, theta = 0.1, gamma=0.3, verbose=True)\n",
    "# x_test_JSMA_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d730b68",
   "metadata": {},
   "source": [
    "*Basic Iterative Method (BMI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3987b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adv_crafter = BasicIterativeMethod(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_BIM_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_BIM_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76558c01",
   "metadata": {},
   "source": [
    "*Projected Gradient Descent (PGD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64259af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = ProjectedGradientDescent(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_PGD_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_PGD_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f431",
   "metadata": {},
   "source": [
    "*NewtonFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter =  NewtonFool(classifier=classifier, eta=0.005, max_iter=25, verbose=True)\n",
    "# x_test_Newton_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Newton_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5111d",
   "metadata": {},
   "source": [
    "*DeepFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca35c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = DeepFool(classifier=classifier, epsilon=1e-06/255, max_iter=50)\n",
    "# x_test_Deep_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Deep_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f9891",
   "metadata": {},
   "source": [
    "*Adversarial Examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA_MNIST\n",
    "x_test_adv = x_test_JSMA_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4a5f",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the classifier on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee462839",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_adv = classifier.predict(x_test_adv)\n",
    "accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy_adv * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6a1c",
   "metadata": {},
   "source": [
    "Step 5: Split the adversarial test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214595e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_adv_indexes=[]\n",
    "fp_adv_indexes=[]\n",
    "x_test_adv_tp=[]\n",
    "y_test_adv_tp=[]\n",
    "x_test_adv_fp=[]\n",
    "y_test_adv_fp=[]\n",
    "\n",
    "for k in range(len(predictions_adv)):\n",
    "    if(np.argmax(predictions_adv, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_adv_indexes.append(k)\n",
    "    else:\n",
    "        fp_adv_indexes.append(k)\n",
    "\n",
    "for k in tp_adv_indexes:\n",
    "    x_test_adv_tp.append(x_test_adv[k])\n",
    "    y_test_adv_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_adv_indexes:\n",
    "    x_test_adv_fp.append(x_test_adv[k])\n",
    "    y_test_adv_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_adv_tp = np.array(x_test_adv_tp)\n",
    "x_test_adv_fp = np.array(x_test_adv_fp)\n",
    "\n",
    "print('Adversarial TP: {:}'.format(len(x_test_adv_tp)))\n",
    "print('Adversarial FP: {:}'.format(len(x_test_adv_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3b44",
   "metadata": {},
   "source": [
    "Optional step: Plot benign samples and their adversarial counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb593b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "num = 3\n",
    "\n",
    "for i in range(num):\n",
    "    ax = plt.subplot(4, num, i + 1)\n",
    "    plt.imshow(x_test_cln[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(y_test_cln,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, num, i + num + 1)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(predictions_adv,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800d61",
   "metadata": {},
   "source": [
    "## **Section 2 - Defence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d97aeb",
   "metadata": {},
   "source": [
    "### **PixelDefend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f8fb1",
   "metadata": {},
   "source": [
    "Step 1: Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "737293cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=5, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d448e021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4dd634bdb1843899d5677e3e10efee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PixelDefend:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_test_adv = x_test_adv.astype(np.float32)\n",
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a341d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-87b552416341>:6: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  ax = plt.subplot(4, num/2, i + 1)\n",
      "<ipython-input-8-87b552416341>:10: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  ax = plt.subplot(4, num/2, i + num + 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "num = 10\n",
    "\n",
    "for i in range(num):\n",
    "    ax = plt.subplot(4, num/2, i + 1)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, num/2, i + num + 1)\n",
    "    plt.imshow(x_test_adv_pd[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pd = defence(x_test_cln*255)[0] / 255\n",
    "x_test_cln_tp_pd = defence(x_test_cln_tp * 255)[0] / 255\n",
    "x_test_cln_fp_pd = defence(x_test_cln_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255\n",
    "x_test_adv_tp_pd = defence(x_test_adv_tp * 255)[0] / 255\n",
    "x_test_adv_fp_pd = defence(x_test_adv_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f6b8",
   "metadata": {},
   "source": [
    "Step 2: Evaluate the classifier on all 4 sets of data after PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab56c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_cln_pd = classifier.predict(x_test_cln_pd)\n",
    "accuracy_cln_pd = np.sum(np.argmax(predictions_cln_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Effect of PixelDefend on entire clean test set: {:.2f}%\".format((accuracy_cln_pd - accuracy_cln) * 100))\n",
    " \n",
    "predictions_cln_tp_pd = classifier.predict(x_test_cln_tp_pd)\n",
    "accuracy_cln_tp_pd = np.sum(np.argmax(predictions_cln_tp_pd, axis=1) == np.argmax(y_test_cln_tp, axis=1)) / len(y_test_cln_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive clean test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_cln_tp_pd) * 100))\n",
    "\n",
    "predictions_cln_fp_pd = classifier.predict(x_test_cln_fp_pd)\n",
    "accuracy_cln_fp_pd = np.sum(np.argmax(predictions_cln_fp_pd, axis=1) == np.argmax(y_test_cln_fp, axis=1)) / len(y_test_cln_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_fp_pd * 100))\n",
    "\n",
    "predictions_adv_pd = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv_pd = np.sum(np.argmax(predictions_adv_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"\\nEffect of PixelDefend on entire adversarial test set: {:.2f}%\".format((accuracy_adv_pd-accuracy_adv) * 100))\n",
    "\n",
    "predictions_adv_tp_pd = classifier.predict(x_test_adv_tp_pd)\n",
    "accuracy_adv_tp_pd = np.sum(np.argmax(predictions_adv_tp_pd, axis=1) == np.argmax(y_test_adv_tp, axis=1)) / len(y_test_adv_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive adversarial test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_adv_tp_pd) * 100))\n",
    "\n",
    "predictions_adv_fp_pd = classifier.predict(x_test_adv_fp_pd)\n",
    "accuracy_adv_fp_pd = np.sum(np.argmax(predictions_adv_fp_pd, axis=1) == np.argmax(y_test_adv_fp, axis=1)) / len(y_test_adv_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_fp_pd * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8236f2",
   "metadata": {},
   "source": [
    "Optional step: Plot all data pre- and post-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee10bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot images\n",
    "predictions_cln_tp = classifier.predict(x_test_cln_tp)\n",
    "predictions_cln_fp = classifier.predict(x_test_cln_fp)\n",
    "predictions_adv_tp = classifier.predict(x_test_adv_tp)\n",
    "predictions_adv_fp = classifier.predict(x_test_adv_fp)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#Plot clean true positives\n",
    "ax = plt.subplot(4, 2, 2*0+1)\n",
    "plt.imshow(x_test_cln_tp[0], cmap='gray')\n",
    "ax.set_title('Clean TP: {:}'.format(np.argmax(predictions_cln_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*0+2)\n",
    "plt.imshow(x_test_cln_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean TP after PixelDefend: {:}'.format(np.argmax(predictions_cln_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot clean false positives\n",
    "ax = plt.subplot(4, 2, 2*1+1)\n",
    "plt.imshow(x_test_cln_fp[0], cmap='gray')\n",
    "ax.set_title('Clean FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]), fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*1+2)\n",
    "plt.imshow(x_test_cln_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp_pd,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial true positives\n",
    "ax = plt.subplot(4, 2, 2*2+1)\n",
    "plt.imshow(x_test_adv_tp[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP: {:}'.format(np.argmax(predictions_adv_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*2+2)\n",
    "plt.imshow(x_test_adv_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP after PixelDefend: {:}'.format(np.argmax(predictions_adv_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial false positivies\n",
    "ax = plt.subplot(4, 2, 2*3+1)\n",
    "plt.imshow(x_test_adv_fp[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*3+2)\n",
    "plt.imshow(x_test_adv_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp_pd,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21800af9",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c630",
   "metadata": {},
   "source": [
    "Optional step: Compare the performance of TotalVarMin against the adversary over a range of eps values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# accuracy_original = []\n",
    "# accuracy_robust = []\n",
    "\n",
    "# adv_crafter = FastGradientMethod(classifier)\n",
    "# adv_crafter_robust = FastGradientMethod(robust_classifier)\n",
    "\n",
    "# for eps in eps_range:\n",
    "#     adv_crafter.set_params(**{'eps': eps})\n",
    "#     adv_crafter_robust.set_params(**{'eps': eps})\n",
    "#     x_test_adv = adv_crafter.generate(x_test[:100])\n",
    "#     x_test_adv_robust = adv_crafter_robust.generate(x_test[:100])\n",
    "    \n",
    "#     predictions_original = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "#     accuracy_original += [np.sum(predictions_original == np.argmax(y_test[:100], axis=1))]\n",
    "    \n",
    "#     predictions_robust = np.argmax(robust_classifier.predict(x_test_adv_robust), axis=1)\n",
    "#     accuracy_robust += [np.sum(predictions_robust == np.argmax(y_test[:100], axis=1))]\n",
    "\n",
    "# eps_range = eps_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cbbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_original), 'b--', label='Original classifier')\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_robust), 'r--', label='Robust classifier')\n",
    "\n",
    "# legend = ax.legend(loc='upper right', shadow=True, fontsize='large')\n",
    "# #legend.get_frame().set_facecolor('#00FFCC')\n",
    "\n",
    "# plt.xlabel('Attack strength (eps)')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42be54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from art.estimators.classification.pytorch import PyTorchClassifier\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "\n",
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5a9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from art.estimators.classification.pytorch import PyTorchClassifier\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.utils import load_mnist\n",
    "\n",
    "from tests.utils import master_seed\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(28*28, 28*28*256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 28, 28, 1, 256)\n",
    "\n",
    "        return logit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c3c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model = PixelCNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "pixelcnn = PyTorchClassifier(\n",
    "    model=model, loss=loss_fn, optimizer=optimizer, input_shape=(28, 28, 1), nb_classes=10, clip_values=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70ae209",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cln = x_train_cln.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d7151a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected target size (64, 28, 1, 256), got torch.Size([64])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c47c8faad721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpixelcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_cln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/classifier.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0;31m# Form the loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# lgtm [py/call-to-non-callable]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;31m# Do training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected target size {}, got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (64, 28, 1, 256), got torch.Size([64])"
     ]
    }
   ],
   "source": [
    "pixelcnn.fit(x_train_cln, y_train_cln, 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ece746e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected target size (64, 28, 1, 256), got torch.Size([64, 1, 28, 28])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d412941c88d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected target size {}, got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (64, 28, 1, 256), got torch.Size([64, 1, 28, 28])"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "mnist_data = datasets.MNIST(root=r'/home/cyber/Desktop/Adrian', train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "train_loader = DataLoader(mnist_data, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, N_EPOCHS, loss.data[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb00e6",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MaskedCNN import MaskedCNN\n",
    "import torch.nn as nn\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tNetwork of PixelCNN as described in A Oord et. al. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, no_layers=8, kernel = 7, channels=64, device=None):\n",
    "\t\tsuper(PixelCNN, self).__init__()\n",
    "\t\tself.no_layers = no_layers\n",
    "\t\tself.kernel = kernel\n",
    "\t\tself.channels = channels\n",
    "\t\tself.layers = {}\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.Conv2d_1 = MaskedCNN('A',1,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_1 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_1= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_2 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_2 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_2= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_3 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_3 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_3= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_4 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_4 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_4= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_5 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_5 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_5= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_6 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_6 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_6= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_7 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_7 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_7= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_8 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_8 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_8= nn.ReLU(True)\n",
    "\n",
    "\t\tself.out = nn.Conv2d(channels, 256, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "# \t\tx = torch.mean(x,dim=1,keepdim=True)\n",
    "        \n",
    "\t\tx = self.Conv2d_1(x)\n",
    "\t\tx = self.BatchNorm2d_1(x)\n",
    "\t\tx = self.ReLU_1(x)\n",
    "\n",
    "\t\tx = self.Conv2d_2(x)\n",
    "\t\tx = self.BatchNorm2d_2(x)\n",
    "\t\tx = self.ReLU_2(x)\n",
    "\n",
    "\t\tx = self.Conv2d_3(x)\n",
    "\t\tx = self.BatchNorm2d_3(x)\n",
    "\t\tx = self.ReLU_3(x)\n",
    "\n",
    "\t\tx = self.Conv2d_4(x)\n",
    "\t\tx = self.BatchNorm2d_4(x)\n",
    "\t\tx = self.ReLU_4(x)\n",
    "\n",
    "\t\tx = self.Conv2d_5(x)\n",
    "\t\tx = self.BatchNorm2d_5(x)\n",
    "\t\tx = self.ReLU_5(x)\n",
    "\n",
    "\t\tx = self.Conv2d_6(x)\n",
    "\t\tx = self.BatchNorm2d_6(x)\n",
    "\t\tx = self.ReLU_6(x)\n",
    "\n",
    "\t\tx = self.Conv2d_7(x)\n",
    "\t\tx = self.BatchNorm2d_7(x)\n",
    "\t\tx = self.ReLU_7(x)\n",
    "\n",
    "\t\tx = self.Conv2d_8(x)\n",
    "\t\tx = self.BatchNorm2d_8(x)\n",
    "\t\tx = self.ReLU_8(x)\n",
    "\n",
    "\t\treturn self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "model.load_state_dict(torch.load(\"/home/cyber/miniconda3/envs/tf-gpu/PixelCNN-Pytorch-master/Models/Model_Checkpoint_Last.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca08bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "pixelcnn = PyTorchClassifier(model=model, loss=loss_fn, optimizer=optimizer, input_shape=(28, 28, 1), nb_classes=10, clip_values=(0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84356fc1",
   "metadata": {},
   "source": [
    "Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a471c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Implementation by jzbontar/pixelcnn-pytorch\n",
    "    \n",
    "    mask_type: must be 'A' or 'B' (see [1])\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in ['A', 'B']\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        h = self.weight.size()[2]\n",
    "        w = self.weight.size()[3]\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, h // 2, w // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, h // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "    \n",
    "\n",
    "class GatedMaskedConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(GatedMaskedConv2d, self).__init__()\n",
    "        self.masked_conv_1 = MaskedConv2d(*args, **kwargs)\n",
    "        self.masked_conv_2 = MaskedConv2d(*args, **kwargs)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        \"\"\"\n",
    "        inp = self.tanh(self.masked_conv_1(x))\n",
    "        gate = self.sigm(self.masked_conv_2(x))\n",
    "        return inp*gate\n",
    "\n",
    "    \n",
    "\n",
    "class CondGatedMaskedConv2d(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CondGatedMaskedConv2d, self).__init__()\n",
    "        self.masked_conv_1 = MaskedConv2d(*args, **kwargs)\n",
    "        self.masked_conv_2 = MaskedConv2d(*args, **kwargs)\n",
    "        self.cond_conv_1 = nn.Conv2d(1, args[2], 1)\n",
    "        self.cond_conv_2 = nn.Conv2d(1, args[2], 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "        x: input\n",
    "        h: conditional input (should have the same shape as input)\n",
    "        \"\"\"\n",
    "        inp = self.tanh(self.masked_conv_1(x)\n",
    "                        + self.cond_conv_1(h))\n",
    "        gate = self.sigm(self.masked_conv_2(x)\n",
    "                         + self.cond_conv_2(h))\n",
    "        return inp * gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456a4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional PixelCNN using gated convolutional layers\n",
    "    \n",
    "    n_channels: channels for each convolutional layer\n",
    "    n_layers: number of intermediate convolutional layers\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=32, n_layers=7):\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(CondGatedMaskedConv2d('A', 1, n_channels,\n",
    "                                                 7, 1, 3, bias=False))\n",
    "        self.layers.append(nn.BatchNorm2d(n_channels))\n",
    "        \n",
    "        for i in range(1, n_layers+1):\n",
    "            self.layers.append(CondGatedMaskedConv2d('B', n_channels,\n",
    "                                                     n_channels, 7, 1, 3,\n",
    "                                                     bias=False))\n",
    "            self.layers.append(nn.BatchNorm2d(n_channels))\n",
    "\n",
    "        # map to 256 output channels\n",
    "        self.layers.append(nn.Conv2d(n_channels, 256, 1))\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            # only pass conditional input to the CondGatedMaskedConv2d layers\n",
    "            if isinstance(layer, CondGatedMaskedConv2d):\n",
    "                out = layer(out, h)\n",
    "            else:\n",
    "                out = layer(out)\n",
    "        return out\n",
    "                              \n",
    "\n",
    "class LabelNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer to map from one-hot-encoded label to 28x28\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape=10, output_shape=(28,28)):\n",
    "        super(LabelNet, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.linear = nn.Linear(10, np.prod(output_shape))\n",
    "\n",
    "    def forward(self, h):\n",
    "        return self.linear(h).view(-1, 1, *self.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "testloader = data.DataLoader(datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor()), batch_size=128, shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa088a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "\n",
    "n_classes = 10 # number of classes\n",
    "n_epochs = 3 # number of epochs to train\n",
    "n_layers = 7 # number of convolutional layers\n",
    "n_channels = 16 # number of channels\n",
    "device = 'cuda:0'\n",
    "\n",
    "def to_one_hot(y, k=10):\n",
    "    y = y.view(-1, 1)\n",
    "    y_one_hot = torch.zeros(y.numel(), k)\n",
    "    y_one_hot.scatter_(1, y, 1)\n",
    "    return y_one_hot.float()\n",
    "\n",
    "pixel_cnn = PixelCNN(n_channels, n_layers).to(device)\n",
    "label_net = LabelNet().to(device)\n",
    "\n",
    "sample = torch.Tensor(120, 1, 28, 28).to(device)\n",
    "optimizer = optim.Adam(list(pixel_cnn.parameters())+\n",
    "                       list(label_net.parameters()))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop from jzbontar/pixelcnn-pytorch\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    err_tr = []\n",
    "    time_tr = time.time()\n",
    "    pixel_cnn.train()\n",
    "    label_net.train()\n",
    "    for inp, lab in trainloader:\n",
    "        lab = to_one_hot(lab)\n",
    "        lab_emb = label_net(lab.to(device))\n",
    "        inp = inp.to(device)\n",
    "        target = (inp[:,0] * 255).long()\n",
    "        loss = criterion(pixel_cnn(inp, lab_emb), target)\n",
    "        err_tr.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    time_tr = time.time() - time_tr\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # compute error on test set\n",
    "        err_te = []\n",
    "        time_te = time.time()\n",
    "        pixel_cnn.eval()\n",
    "        label_net.eval()\n",
    "        for inp, lab in testloader:\n",
    "            lab = to_one_hot(lab)\n",
    "            lab_emb = label_net(lab.to(device))\n",
    "            inp = inp.to(device)\n",
    "            target = (inp[:,0] * 255).long()\n",
    "            loss = criterion(pixel_cnn(inp, lab_emb), target)\n",
    "            err_te.append(loss.item())\n",
    "        time_te = time.time() - time_te\n",
    "\n",
    "        # sample\n",
    "        labels = torch.arange(10).repeat(12,1).flatten()\n",
    "        sample.fill_(0)\n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                out = pixel_cnn(sample, label_net(to_one_hot(labels).to(device)))\n",
    "                probs = F.softmax(out[:, :, i, j], dim=1)\n",
    "                sample[:, :, i, j] = torch.multinomial(probs, 1).float() / 255.\n",
    "\n",
    "        utils.save_image(sample, 'sample_{:02d}.png'.format(epoch+1), nrow=10, padding=0)\n",
    "\n",
    "        output_string = 'epoch: {}/{} bpp (train): {:.7f}' + \\\n",
    "            ' bpp (test): {:.7f} time (training): {:.1f}s time (testing): {:.1f}s'\n",
    "        print(output_string.format(epoch+1,\n",
    "                                   n_epochs,\n",
    "                                   np.mean(err_tr)/np.log(2),\n",
    "                                   np.mean(err_te)/np.log(2),\n",
    "                                   time_tr,\n",
    "                                   time_te))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

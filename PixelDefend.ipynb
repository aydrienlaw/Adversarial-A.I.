{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256084fa",
   "metadata": {},
   "source": [
    "**This notebook focuses on the effectiveness of Total Variance Minimization against adversarial attacks on the MNIST and MARVEL datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdf42",
   "metadata": {},
   "source": [
    "## **Section 0 - Setting Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbf7b5",
   "metadata": {},
   "source": [
    "### **Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d608e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, Layer\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from art import config\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, ProjectedGradientDescent, SaliencyMapMethod, CarliniL2Method, NewtonFool, BasicIterativeMethod\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier, TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2e0d4",
   "metadata": {},
   "source": [
    "### Load PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dc5a9676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import unittest\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from art.estimators.classification.pytorch import PyTorchClassifier\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.utils import load_mnist\n",
    "\n",
    "from tests.utils import master_seed\n",
    "\n",
    "\n",
    "class ModelImage(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelImage, self).__init__()\n",
    "        self.fc = nn.Linear(25, 6400)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28*128)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 5, 5, 1, 256)\n",
    "\n",
    "        return logit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "11c3c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "model = ModelImage()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "pixelcnn = PyTorchClassifier(\n",
    "    model=model, loss=loss_fn, optimizer=optimizer, input_shape=(1, 28, 28), nb_classes=10, clip_values=(0, 1)\n",
    ")\n",
    "defence = PixelDefend(eps=5, pixel_cnn=pixelcnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "aeeffc96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-2835f8a774d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_defended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_cln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/defences/preprocessor/pixel_defend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0moriginal_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             probs = self.pixel_cnn.get_activations(x, layer=-1, batch_size=self.batch_size).reshape(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mget_activations\u001b[0;34m(self, x, layer, batch_size, framework)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;31m# Run prediction for the current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_preprocessed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m                         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-137-ef7ba9ffdecf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlogit_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlogit_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "x_defended = defence(x_test_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87b3e29",
   "metadata": {},
   "source": [
    "### Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e711de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_as_image(binary_image, figsize=(10, 5)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(binary_image, cmap='gray')\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7aed5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9f0a59c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAAEhCAYAAAAwHRYbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEAUlEQVR4nO3XsU1DQRBAwX/IJZiYXwT9V2AXQe4elgwJywFGwJPMTLjaYKOnuzUzG0DlqT4A+N9ECEiJEJASISAlQkBKhIDU4Z7l4/E4+77/0inAIzufz5eZeb6e3xWhfd+30+n0c1cB/8Za6+3W3HcMSIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJA61AfAd6y16hP4IV5CQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKTWzHx9ea2vLwN8dp6Z1+uhlxCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgNThzv3Ltm1vv3EI8PBebg3XzPz1IQAffMeAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgNQ7G4cd1ou5gpkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASEAAAEhCAYAAAAwHRYbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEAUlEQVR4nO3XMUpDURBA0f8kS4i1fxHufwXJIuzdw9gJhiAJGC/oOeUwxVSX99bMbACVp/oA4H8TISAlQkBKhICUCAEpEQJSh3uWj8fj7Pv+oFOAv+x8Pr/PzPPl/K4I7fu+nU6nn7sK+DfWWm/X5r5jQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhIDUoT6Ax1pr1SfAt7yEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEitmbl9ea3blwG+Os/M6+XQSwhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgJQIASkRAlIiBKRECEiJEJASISAlQkBKhICUCAEpEQJSIgSkRAhIiRCQEiEgJUJASoSAlAgBKRECUiIEpEQISIkQkBIhICVCQOpw5/77tm1vjzgE+PNerg3XzPz2IQCffMeAlAgBKRECUiIEpEQISIkQkBIhICVCQEqEgNQHHg8d1kI0CYsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "show_as_image(conv_mask(5, 5, include_center=True))\n",
    "\n",
    "show_as_image(conv_mask(5, 5, include_center=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1bb2824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.,  2.,  3.],\n",
       "         [ 4.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[10., 11., 12.],\n",
       "         [13.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]],\n",
       "\n",
       "\n",
       "       [[[19., 20., 21.],\n",
       "         [22.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]],\n",
       "\n",
       "        [[28., 29., 30.],\n",
       "         [31.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.]]]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels, in_channels, width, height = 2, 2, 3, 3\n",
    "\n",
    "conv_weights = 1 + np.arange(out_channels * in_channels * width * height).reshape((out_channels, in_channels, width, height))\n",
    "\n",
    "masked_weights = conv_weights * conv_mask(width, height)\n",
    "\n",
    "masked_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc0c4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2aaadbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ddb877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(root=r'/home/cyber/Desktop/Adrian', train=True, download=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0d06e978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 2 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-9dc483d88577>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2388\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2390\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2391\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2392\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 2 is out of bounds."
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.01\n",
    "\n",
    "cnn = PixelCNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)\n",
    "\n",
    "train_loader = DataLoader(mnist_data, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=cnn(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, N_EPOCHS, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0f0b",
   "metadata": {},
   "source": [
    "### **Modification: Disabling eager execution to enable adversarial crafting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87648805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8b7ec",
   "metadata": {},
   "source": [
    "### **Load MARVEL dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test_cln = []\n",
    "y_test_cln = [] \n",
    "min_pixel_value = 0\n",
    "max_pixel_value = 1\n",
    "\n",
    "def marvel_class(filename):\n",
    "    switcher={\n",
    "        'HeavyLoadCarrier': [1,0,0,0,0,0,0,0,0],\n",
    "        'CombatVessel': [0,1,0,0,0,0,0,0,0],\n",
    "        'ContainerShip': [0,0,1,0,0,0,0,0,0],\n",
    "        'PassengersShip': [0,0,0,1,0,0,0,0,0],\n",
    "        'Ro-roCargo': [0,0,0,0,1,0,0,0,0],\n",
    "        'Tanker': [0,0,0,0,0,1,0,0,0],\n",
    "        'Tug': [0,0,0,0,0,0,1,0,0],\n",
    "        'SupplyVessel': [0,0,0,0,0,0,0,1,0],\n",
    "        'Yacht': [0,0,0,0,0,0,0,0,1]\n",
    "    }\n",
    "    return switcher.get(filename)\n",
    "\n",
    "def load_training_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/train_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_train_cln.append(img/255)\n",
    "            y_train_cln.append(marvel_class(filename))\n",
    "            i = i+1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_train_cln, y_train_cln\n",
    "\n",
    "def load_test_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/test_9/\"+filename\n",
    "    i = 0\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_test_cln.append(img/255)\n",
    "            y_test_cln.append(marvel_class(filename))\n",
    "            i = i + 1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_test_cln, y_test_cln\n",
    "\n",
    "# for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/train_9\"):\n",
    "#     load_training_data(filename)\n",
    "#     print(filename)\n",
    "\n",
    "for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"):\n",
    "    load_test_data(filename)\n",
    "    print(filename)\n",
    "    \n",
    "#load_training_data(\"/home/cyber/Desktop/Adrian/marvel_data/test_9/CombatVessel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6df68",
   "metadata": {},
   "source": [
    "*Modification: Convert MARVEL x_test/x_train from uint8 into float32, to enable classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln = np.array(x_test_cln, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58648ad",
   "metadata": {},
   "source": [
    "### **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2aa87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_cln, y_train_cln), (x_test_cln, y_test_cln), min_pixel_value, max_pixel_value = load_mnist()\n",
    "# x_test_cln, y_test_cln = x_test_cln[:1000], y_test_cln[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c4a58",
   "metadata": {},
   "source": [
    "### **Load / Create classifier model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c095be",
   "metadata": {},
   "source": [
    "*MNIST pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8ddd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"/home/cyber/mnist_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa596aeb",
   "metadata": {},
   "source": [
    "*MARVEL pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab83ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/cyber/Desktop/Adrian/Xception-10-0.74.hdf5\"\n",
    "model = load_model(model_path, custom_objects={'RAdam': RAdam}, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f6bd9",
   "metadata": {},
   "source": [
    "*Optional step: Train and save a model for future use*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_cln, y_train_cln, batch_size=64, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "399ff0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/cyber/dataset_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2df92",
   "metadata": {},
   "source": [
    "*Create ART classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4d04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc077ab",
   "metadata": {},
   "source": [
    "## **Section 1 - Attack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9dd4",
   "metadata": {},
   "source": [
    "Step 1: Evaluate the classifier on benign test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1362d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyber/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 99.11%\n"
     ]
    }
   ],
   "source": [
    "predictions_cln = classifier.predict(x_test_cln)\n",
    "accuracy_cln = np.sum(np.argmax(predictions_cln, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy_cln * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac38ca",
   "metadata": {},
   "source": [
    "Step 2: Split benign test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7909c97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of benign true positives: 9911\n",
      "Number of benign false positives: 89\n"
     ]
    }
   ],
   "source": [
    "tp_cln_indexes=[]\n",
    "fp_cln_indexes=[]\n",
    "x_test_cln_tp=[]\n",
    "y_test_cln_tp=[]\n",
    "x_test_cln_fp=[]\n",
    "y_test_cln_fp=[]\n",
    "\n",
    "for k in range(len(predictions_cln)):\n",
    "    if(np.argmax(predictions_cln, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_cln_indexes.append(k)\n",
    "    else:\n",
    "        fp_cln_indexes.append(k)\n",
    "\n",
    "for k in tp_cln_indexes:\n",
    "    x_test_cln_tp.append(x_test_cln[k])\n",
    "    y_test_cln_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_cln_indexes:\n",
    "    x_test_cln_fp.append(x_test_cln[k])\n",
    "    y_test_cln_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_cln_tp = np.array(x_test_cln_tp)\n",
    "x_test_cln_fp = np.array(x_test_cln_fp)\n",
    "\n",
    "print('Number of benign true positives: {:}'.format(len(x_test_cln_tp)))\n",
    "print('Number of benign false positives: {:}'.format(len(x_test_cln_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b91dd",
   "metadata": {},
   "source": [
    "Step 3: Craft adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e07528",
   "metadata": {},
   "source": [
    "*Jacobian-based Saliency Map Attack (JSMA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ffc57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = SaliencyMapMethod(classifier=classifier, theta = 0.1, gamma=0.3, verbose=True)\n",
    "# x_test_JSMA_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d730b68",
   "metadata": {},
   "source": [
    "*Basic Iterative Method (BMI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a3987b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adv_crafter = BasicIterativeMethod(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_BIM_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_BIM_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76558c01",
   "metadata": {},
   "source": [
    "*Projected Gradient Descent (PGD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64259af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = ProjectedGradientDescent(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_PGD_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_PGD_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f431",
   "metadata": {},
   "source": [
    "*NewtonFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57cf574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter =  NewtonFool(classifier=classifier, eta=0.005, max_iter=25, verbose=True)\n",
    "# x_test_Newton_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Newton_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5111d",
   "metadata": {},
   "source": [
    "*DeepFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fca35c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = DeepFool(classifier=classifier, epsilon=1e-06/255, max_iter=50)\n",
    "# x_test_Deep_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Deep_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f9891",
   "metadata": {},
   "source": [
    "*Adversarial Examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d6ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA_MNIST\n",
    "x_test_adv = x_test_JSMA_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4a5f",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the classifier on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee462839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 0.08%\n"
     ]
    }
   ],
   "source": [
    "predictions_adv = classifier.predict(x_test_adv)\n",
    "accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy_adv * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6a1c",
   "metadata": {},
   "source": [
    "Step 5: Split the adversarial test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "214595e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial TP: 8\n",
      "Adversarial FP: 9992\n"
     ]
    }
   ],
   "source": [
    "tp_adv_indexes=[]\n",
    "fp_adv_indexes=[]\n",
    "x_test_adv_tp=[]\n",
    "y_test_adv_tp=[]\n",
    "x_test_adv_fp=[]\n",
    "y_test_adv_fp=[]\n",
    "\n",
    "for k in range(len(predictions_adv)):\n",
    "    if(np.argmax(predictions_adv, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_adv_indexes.append(k)\n",
    "    else:\n",
    "        fp_adv_indexes.append(k)\n",
    "\n",
    "for k in tp_adv_indexes:\n",
    "    x_test_adv_tp.append(x_test_adv[k])\n",
    "    y_test_adv_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_adv_indexes:\n",
    "    x_test_adv_fp.append(x_test_adv[k])\n",
    "    y_test_adv_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_adv_tp = np.array(x_test_adv_tp)\n",
    "x_test_adv_fp = np.array(x_test_adv_fp)\n",
    "\n",
    "print('Adversarial TP: {:}'.format(len(x_test_adv_tp)))\n",
    "print('Adversarial FP: {:}'.format(len(x_test_adv_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3b44",
   "metadata": {},
   "source": [
    "Optional step: Plot benign samples and their adversarial counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9fb593b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "num = 3\n",
    "\n",
    "for i in range(num):\n",
    "    ax = plt.subplot(4, num, i + 1)\n",
    "    plt.imshow(x_test_cln[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(y_test_cln,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, num, i + num + 1)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(predictions_adv,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800d61",
   "metadata": {},
   "source": [
    "## **Section 2 - Defence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d97aeb",
   "metadata": {},
   "source": [
    "### **PixelDefend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f8fb1",
   "metadata": {},
   "source": [
    "Step 1: Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "737293cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=5, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2b49039",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pd = defence(x_test_cln*255)[0] / 255\n",
    "x_test_cln_tp_pd = defence(x_test_cln_tp * 255)[0] / 255\n",
    "x_test_cln_fp_pd = defence(x_test_cln_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255\n",
    "x_test_adv_tp_pd = defence(x_test_adv_tp * 255)[0] / 255\n",
    "x_test_adv_fp_pd = defence(x_test_adv_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f6b8",
   "metadata": {},
   "source": [
    "Step 2: Evaluate the classifier on all 4 sets of data after PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab56c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_cln_pd = classifier.predict(x_test_cln_pd)\n",
    "accuracy_cln_pd = np.sum(np.argmax(predictions_cln_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Effect of PixelDefend on entire benign test set: {:.2f}%\".format((accuracy_cln_pd - accuracy_cln) * 100))\n",
    " \n",
    "predictions_cln_tp_pd = classifier.predict(x_test_cln_tp_pd)\n",
    "accuracy_cln_tp_pd = np.sum(np.argmax(predictions_cln_tp_pd, axis=1) == np.argmax(y_test_cln_tp, axis=1)) / len(y_test_cln_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive benign test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive benign test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_cln_tp_pd) * 100))\n",
    "\n",
    "predictions_cln_fp_pd = classifier.predict(x_test_cln_fp_pd)\n",
    "accuracy_cln_fp_pd = np.sum(np.argmax(predictions_cln_fp_pd, axis=1) == np.argmax(y_test_cln_fp, axis=1)) / len(y_test_cln_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive benign test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_fp_pd * 100))\n",
    "\n",
    "predictions_adv_pd = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv_pd = np.sum(np.argmax(predictions_adv_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"\\nEffect of PixelDefend on entire adversarial test set: {:.2f}%\".format((accuracy_adv_pd-accuracy_adv) * 100))\n",
    "\n",
    "predictions_adv_tp_pd = classifier.predict(x_test_adv_tp_pd)\n",
    "accuracy_adv_tp_pd = np.sum(np.argmax(predictions_adv_tp_pd, axis=1) == np.argmax(y_test_adv_tp, axis=1)) / len(y_test_adv_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive adversarial test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_adv_tp_pd) * 100))\n",
    "\n",
    "predictions_adv_fp_pd = classifier.predict(x_test_adv_fp_pd)\n",
    "accuracy_adv_fp_pd = np.sum(np.argmax(predictions_adv_fp_pd, axis=1) == np.argmax(y_test_adv_fp, axis=1)) / len(y_test_adv_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_fp_pd * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8236f2",
   "metadata": {},
   "source": [
    "Optional step: Plot all data pre- and post-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee10bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot images\n",
    "predictions_cln_tp = classifier.predict(x_test_cln_tp)\n",
    "predictions_cln_fp = classifier.predict(x_test_cln_fp)\n",
    "predictions_adv_tp = classifier.predict(x_test_adv_tp)\n",
    "predictions_adv_fp = classifier.predict(x_test_adv_fp)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#Plot benign true positives\n",
    "ax = plt.subplot(4, 2, 2*0+1)\n",
    "plt.imshow(x_test_cln_tp[0], cmap='gray')\n",
    "ax.set_title('Benign TP: {:}'.format(np.argmax(predictions_cln_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*0+2)\n",
    "plt.imshow(x_test_cln_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Benign TP after PixelDefend: {:}'.format(np.argmax(predictions_cln_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot benign false positives\n",
    "ax = plt.subplot(4, 2, 2*1+1)\n",
    "plt.imshow(x_test_cln_fp[0], cmap='gray')\n",
    "ax.set_title('Benign FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]), fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*1+2)\n",
    "plt.imshow(x_test_cln_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Benign FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp_pd,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial true positives\n",
    "ax = plt.subplot(4, 2, 2*2+1)\n",
    "plt.imshow(x_test_adv_tp[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP: {:}'.format(np.argmax(predictions_adv_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*2+2)\n",
    "plt.imshow(x_test_adv_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP after PixelDefend: {:}'.format(np.argmax(predictions_adv_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial false positivies\n",
    "ax = plt.subplot(4, 2, 2*3+1)\n",
    "plt.imshow(x_test_adv_fp[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*3+2)\n",
    "plt.imshow(x_test_adv_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp_pd,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21800af9",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c630",
   "metadata": {},
   "source": [
    "Optional step: Compare the performance of TotalVarMin against the adversary over a range of eps values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# accuracy_original = []\n",
    "# accuracy_robust = []\n",
    "\n",
    "# adv_crafter = FastGradientMethod(classifier)\n",
    "# adv_crafter_robust = FastGradientMethod(robust_classifier)\n",
    "\n",
    "# for eps in eps_range:\n",
    "#     adv_crafter.set_params(**{'eps': eps})\n",
    "#     adv_crafter_robust.set_params(**{'eps': eps})\n",
    "#     x_test_adv = adv_crafter.generate(x_test[:100])\n",
    "#     x_test_adv_robust = adv_crafter_robust.generate(x_test[:100])\n",
    "    \n",
    "#     predictions_original = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "#     accuracy_original += [np.sum(predictions_original == np.argmax(y_test[:100], axis=1))]\n",
    "    \n",
    "#     predictions_robust = np.argmax(robust_classifier.predict(x_test_adv_robust), axis=1)\n",
    "#     accuracy_robust += [np.sum(predictions_robust == np.argmax(y_test[:100], axis=1))]\n",
    "\n",
    "# eps_range = eps_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cbbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_original), 'b--', label='Original classifier')\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_robust), 'r--', label='Robust classifier')\n",
    "\n",
    "# legend = ax.legend(loc='upper right', shadow=True, fontsize='large')\n",
    "# #legend.get_frame().set_facecolor('#00FFCC')\n",
    "\n",
    "# plt.xlabel('Attack strength (eps)')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412463e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

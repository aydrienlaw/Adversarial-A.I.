{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256084fa",
   "metadata": {},
   "source": [
    "**This notebook focuses on the effectiveness of PixelDefend against adversarial attacks on the MNIST and MARVEL datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdf42",
   "metadata": {},
   "source": [
    "## **Section 0 - Setting Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbf7b5",
   "metadata": {},
   "source": [
    "### **Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d608e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, Layer\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from art import config\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, ProjectedGradientDescent, SaliencyMapMethod, CarliniL2Method, NewtonFool, BasicIterativeMethod\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier, TensorFlowV2Classifier, PyTorchClassifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2e0d4",
   "metadata": {},
   "source": [
    "### Load PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dad960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47726a9c",
   "metadata": {},
   "source": [
    "*Create PixelCNN architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e38fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCNN(nn.Conv2d):\n",
    "\t\"\"\"\n",
    "\tImplementation of Masked CNN Class as explained in A Oord et. al. \n",
    "\tTaken from https://github.com/jzbontar/pixelcnn-pytorch\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, mask_type, *args, **kwargs):\n",
    "\t\tself.mask_type = mask_type\n",
    "\t\tassert mask_type in ['A', 'B'], \"Unknown Mask Type\"\n",
    "\t\tsuper(MaskedCNN, self).__init__(*args, **kwargs)\n",
    "\t\tself.register_buffer('mask', self.weight.data.clone())\n",
    "\n",
    "\t\t_, depth, height, width = self.weight.size()\n",
    "\t\tself.mask.fill_(1)\n",
    "\t\tif mask_type =='A':\n",
    "\t\t\tself.mask[:,:,height//2,width//2:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\t\telse:\n",
    "\t\t\tself.mask[:,:,height//2,width//2+1:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.weight.data*=self.mask\n",
    "\t\treturn super(MaskedCNN, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430768bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tNetwork of PixelCNN as described in A Oord et. al. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, no_layers=8, kernel = 7, channels=64, device=None):\n",
    "\t\tsuper(PixelCNN, self).__init__()\n",
    "\t\tself.no_layers = no_layers\n",
    "\t\tself.kernel = kernel\n",
    "\t\tself.channels = channels\n",
    "\t\tself.layers = {}\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.Conv2d_1 = MaskedCNN('A',1,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_1 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_1= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_2 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_2 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_2= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_3 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_3 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_3= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_4 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_4 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_4= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_5 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_5 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_5= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_6 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_6 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_6= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_7 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_7 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_7= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_8 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_8 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_8= nn.ReLU(True)\n",
    "\n",
    "\t\tself.out = nn.Conv2d(channels, 256, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.Conv2d_1(x)\n",
    "\t\tx = self.BatchNorm2d_1(x)\n",
    "\t\tx = self.ReLU_1(x)\n",
    "\n",
    "\t\tx = self.Conv2d_2(x)\n",
    "\t\tx = self.BatchNorm2d_2(x)\n",
    "\t\tx = self.ReLU_2(x)\n",
    "\n",
    "\t\tx = self.Conv2d_3(x)\n",
    "\t\tx = self.BatchNorm2d_3(x)\n",
    "\t\tx = self.ReLU_3(x)\n",
    "\n",
    "\t\tx = self.Conv2d_4(x)\n",
    "\t\tx = self.BatchNorm2d_4(x)\n",
    "\t\tx = self.ReLU_4(x)\n",
    "\n",
    "\t\tx = self.Conv2d_5(x)\n",
    "\t\tx = self.BatchNorm2d_5(x)\n",
    "\t\tx = self.ReLU_5(x)\n",
    "\n",
    "\t\tx = self.Conv2d_6(x)\n",
    "\t\tx = self.BatchNorm2d_6(x)\n",
    "\t\tx = self.ReLU_6(x)\n",
    "\n",
    "\t\tx = self.Conv2d_7(x)\n",
    "\t\tx = self.BatchNorm2d_7(x)\n",
    "\t\tx = self.ReLU_7(x)\n",
    "\n",
    "\t\tx = self.Conv2d_8(x)\n",
    "\t\tx = self.BatchNorm2d_8(x)\n",
    "\t\tx = self.ReLU_8(x)\n",
    "\n",
    "\t\treturn self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995319e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "# model.load_state_dict(torch.load('/home/cyber/miniconda3/envs/tf-gpu/PixelCNN-Pytorch-master/Models/Model_Checkpoint_Last.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75447b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(28*28, 28*28*256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 256, 28, 28)\n",
    "\n",
    "        return logit_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afb13ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, c_in, c_out, k_size, stride, pad):\n",
    "        \"\"\"2D Convolution with masked weight for Autoregressive connection\"\"\"\n",
    "        super(MaskedConv2d, self).__init__(\n",
    "            c_in, c_out, k_size, stride, pad, bias=False)\n",
    "        assert mask_type in ['A', 'B']\n",
    "        self.mask_type = mask_type\n",
    "        ch_out, ch_in, height, width = self.weight.size()\n",
    "\n",
    "        # Mask\n",
    "        #         -------------------------------------\n",
    "        #        |  1       1       1       1       1 |\n",
    "        #        |  1       1       1       1       1 |\n",
    "        #        |  1       1    1 if B     0       0 |   H // 2\n",
    "        #        |  0       0       0       0       0 |   H // 2 + 1\n",
    "        #        |  0       0       0       0       0 |\n",
    "        #         -------------------------------------\n",
    "        #  index    0       1     W//2    W//2+1\n",
    "\n",
    "        mask = torch.ones(ch_out, ch_in, height, width)\n",
    "        if mask_type == 'A':\n",
    "            # First Convolution Only\n",
    "            # => Restricting connections to\n",
    "            #    already predicted neighborhing channels in current pixel\n",
    "            mask[:, :, height // 2, width // 2:] = 0\n",
    "            mask[:, :, height // 2 + 1:] = 0\n",
    "        else:\n",
    "            mask[:, :, height // 2, width // 2 + 1:] = 0\n",
    "            mask[:, :, height // 2] = 0\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "\n",
    "def maskAConv(c_in=3, c_out=256, k_size=7, stride=1, pad=3):\n",
    "    \"\"\"2D Masked Convolution (type A)\"\"\"\n",
    "    return nn.Sequential(\n",
    "        MaskedConv2d('A', c_in, c_out, k_size, stride, pad),\n",
    "        nn.BatchNorm2d(c_out))\n",
    "\n",
    "\n",
    "class MaskBConvBlock(nn.Module):\n",
    "    def __init__(self, h=128, k_size=3, stride=1, pad=1):\n",
    "        \"\"\"1x1 Conv + 2D Masked Convolution (type B) + 1x1 Conv\"\"\"\n",
    "        super(MaskBConvBlock, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2 * h, h, 1),  # 1x1\n",
    "            nn.BatchNorm2d(h),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', h, h, k_size, stride, pad),\n",
    "            nn.BatchNorm2d(h),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h, 2 * h, 1),  # 1x1\n",
    "            nn.BatchNorm2d(2 * h)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Residual connection\"\"\"\n",
    "        return self.net(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e415788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, n_channel=28, h=128, discrete_channel=256):\n",
    "        \"\"\"PixelCNN Model\"\"\"\n",
    "        super(PixelCNN, self).__init__()\n",
    "\n",
    "        self.discrete_channel = discrete_channel\n",
    "\n",
    "        self.MaskAConv = maskAConv(n_channel, 2 * h, k_size=7, stride=1, pad=3)\n",
    "        MaskBConv = []\n",
    "        for i in range(15):\n",
    "            MaskBConv.append(MaskBConvBlock(h, k_size=3, stride=1, pad=1))\n",
    "        self.MaskBConv = nn.Sequential(*MaskBConv)\n",
    "\n",
    "        # 1x1 conv to 3x256 channels\n",
    "        self.out = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2 * h, 1024, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, n_channel * discrete_channel, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, channel, height, width]\n",
    "        Return:\n",
    "            out [batch_size, channel, height, width, 256]\n",
    "        \"\"\"\n",
    "        batch_size, c_in, height, width = x.size()\n",
    "\n",
    "        # [batch_size, 2h, 32, 32]\n",
    "        x = self.MaskAConv(x)\n",
    "\n",
    "        # [batch_size, 2h, 32, 32]\n",
    "        x = self.MaskBConv(x)\n",
    "\n",
    "        # [batch_size, 3x256, 32, 32]\n",
    "        x = self.out(x)\n",
    "\n",
    "        # [batch_size, 3, 256, 32, 32]\n",
    "        x = x.view(batch_size, c_in, self.discrete_channel, height, width)\n",
    "\n",
    "        # [batch_size, 3, 32, 32, 256]\n",
    "        x = x.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b5c59",
   "metadata": {},
   "source": [
    "*Train PixelCNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "679baad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6170c5a330>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "lr = 0.002\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb900493",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root=r'/home/cyber/Desktop/Adrian', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root=r'/home/cyber/Desktop/Adrian', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b1d1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1825c5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(input=model(data), target=torch.squeeze(data).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(model.state_dict(), '/home/cyber/Desktop/Adrian/results/model.pt')\n",
    "      torch.save(optimizer.state_dict(), '/home/cyber/Desktop/Adrian/results/optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10da6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = model(data)\n",
    "      test_loss += F.cross_entropy(input=model(data), target=torch.squeeze(data).long(), size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93bbbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "411fddbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected target size (128, 28, 28, 256), got torch.Size([128, 28, 28])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2363e51f5fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#   test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-027aa91d24ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2691\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2395\u001b[0m         \u001b[0mout_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected target size {}, got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (128, 28, 28, 256), got torch.Size([128, 28, 28])"
     ]
    }
   ],
   "source": [
    "# test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "#   test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4d44d",
   "metadata": {},
   "source": [
    "*Load PixelCNN*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8eb4d7",
   "metadata": {},
   "source": [
    "*Load PixelCNN into a classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c3ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "pixelcnn = PyTorchClassifier(\n",
    "    model=model, loss=loss_fn, optimizer=optimizer, input_shape=(28, 28, 1), nb_classes=10, clip_values=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb00e6",
   "metadata": {},
   "source": [
    "### Test PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdfdbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA_MNIST\n",
    "x_test_adv = x_test_JSMA_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5b89e99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=5, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c3d749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347f21a33e884596953c807b133edd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PixelDefend:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_adv = x_test_adv.astype(np.float32)\n",
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c16f5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ax = plt.subplot(1,5,i+1)\n",
    "    plt.imshow(x_test_adv_pd[i], cmap='gray')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0f0b",
   "metadata": {},
   "source": [
    "### **Modification: Disabling eager execution to enable adversarial crafting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87648805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8b7ec",
   "metadata": {},
   "source": [
    "### **Load MARVEL dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test_cln = []\n",
    "y_test_cln = [] \n",
    "min_pixel_value = 0\n",
    "max_pixel_value = 1\n",
    "\n",
    "def marvel_class(filename):\n",
    "    switcher={\n",
    "        'HeavyLoadCarrier': [1,0,0,0,0,0,0,0,0],\n",
    "        'CombatVessel': [0,1,0,0,0,0,0,0,0],\n",
    "        'ContainerShip': [0,0,1,0,0,0,0,0,0],\n",
    "        'PassengersShip': [0,0,0,1,0,0,0,0,0],\n",
    "        'Ro-roCargo': [0,0,0,0,1,0,0,0,0],\n",
    "        'Tanker': [0,0,0,0,0,1,0,0,0],\n",
    "        'Tug': [0,0,0,0,0,0,1,0,0],\n",
    "        'SupplyVessel': [0,0,0,0,0,0,0,1,0],\n",
    "        'Yacht': [0,0,0,0,0,0,0,0,1]\n",
    "    }\n",
    "    return switcher.get(filename)\n",
    "\n",
    "def load_training_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/train_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_train_cln.append(img/255)\n",
    "            y_train_cln.append(marvel_class(filename))\n",
    "            i = i+1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_train_cln, y_train_cln\n",
    "\n",
    "def load_test_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/test_9/\"+filename\n",
    "    i = 0\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_test_cln.append(img/255)\n",
    "            y_test_cln.append(marvel_class(filename))\n",
    "            i = i + 1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_test_cln, y_test_cln\n",
    "\n",
    "# for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/train_9\"):\n",
    "#     load_training_data(filename)\n",
    "#     print(filename)\n",
    "\n",
    "for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"):\n",
    "    load_test_data(filename)\n",
    "    print(filename)\n",
    "    \n",
    "#load_training_data(\"/home/cyber/Desktop/Adrian/marvel_data/test_9/CombatVessel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6df68",
   "metadata": {},
   "source": [
    "*Modification: Convert MARVEL x_test/x_train from uint8 into float32, to enable classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln = np.array(x_test_cln, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58648ad",
   "metadata": {},
   "source": [
    "### **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_cln, y_train_cln), (x_test_cln, y_test_cln), min_pixel_value, max_pixel_value = load_mnist()\n",
    "# x_test_cln, y_test_cln = x_test_cln[:1000], y_test_cln[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c4a58",
   "metadata": {},
   "source": [
    "### **Load / Create classifier model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c095be",
   "metadata": {},
   "source": [
    "*MNIST pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8ddd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"/home/cyber/mnist_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa596aeb",
   "metadata": {},
   "source": [
    "*MARVEL pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab83ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/cyber/Desktop/Adrian/Xception-10-0.74.hdf5\"\n",
    "model = load_model(model_path, custom_objects={'RAdam': RAdam}, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f6bd9",
   "metadata": {},
   "source": [
    "*Optional step: Train and save a model for future use*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_cln, y_train_cln, batch_size=64, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ff0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/cyber/dataset_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2df92",
   "metadata": {},
   "source": [
    "*Create ART classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc077ab",
   "metadata": {},
   "source": [
    "## **Section 1 - Attack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9dd4",
   "metadata": {},
   "source": [
    "Step 1: Evaluate the classifier on clean test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln = classifier.predict(x_test_cln)\n",
    "accuracy_cln = np.sum(np.argmax(predictions_cln, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy_cln * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac38ca",
   "metadata": {},
   "source": [
    "Step 2: Split clean test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_cln_indexes=[]\n",
    "fp_cln_indexes=[]\n",
    "x_test_cln_tp=[]\n",
    "y_test_cln_tp=[]\n",
    "x_test_cln_fp=[]\n",
    "y_test_cln_fp=[]\n",
    "\n",
    "for k in range(len(predictions_cln)):\n",
    "    if(np.argmax(predictions_cln, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_cln_indexes.append(k)\n",
    "    else:\n",
    "        fp_cln_indexes.append(k)\n",
    "\n",
    "for k in tp_cln_indexes:\n",
    "    x_test_cln_tp.append(x_test_cln[k])\n",
    "    y_test_cln_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_cln_indexes:\n",
    "    x_test_cln_fp.append(x_test_cln[k])\n",
    "    y_test_cln_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_cln_tp = np.array(x_test_cln_tp)\n",
    "x_test_cln_fp = np.array(x_test_cln_fp)\n",
    "\n",
    "print('Number of benign true positives: {:}'.format(len(x_test_cln_tp)))\n",
    "print('Number of benign false positives: {:}'.format(len(x_test_cln_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b91dd",
   "metadata": {},
   "source": [
    "Step 3: Craft adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e07528",
   "metadata": {},
   "source": [
    "*Jacobian-based Saliency Map Attack (JSMA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = SaliencyMapMethod(classifier=classifier, theta = 0.1, gamma=0.3, verbose=True)\n",
    "# x_test_JSMA_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d730b68",
   "metadata": {},
   "source": [
    "*Basic Iterative Method (BMI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3987b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# adv_crafter = BasicIterativeMethod(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_BIM_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_BIM_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76558c01",
   "metadata": {},
   "source": [
    "*Projected Gradient Descent (PGD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64259af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = ProjectedGradientDescent(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_PGD_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_PGD_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f431",
   "metadata": {},
   "source": [
    "*NewtonFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab72fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter =  NewtonFool(classifier=classifier, eta=0.005, max_iter=25, verbose=True)\n",
    "# x_test_Newton_MNIST = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Newton_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter =  NewtonFool(classifier=classifier, eta=0.005, max_iter=25, verbose=True)\n",
    "# x_test_Newton_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Newton_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5111d",
   "metadata": {},
   "source": [
    "*DeepFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca35c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adv_crafter = DeepFool(classifier=classifier, epsilon=1e-06/255, max_iter=50)\n",
    "# x_test_Deep_MARVEL = adv_crafter.generate(x_test_cln)\n",
    "# %store x_test_Deep_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f9891",
   "metadata": {},
   "source": [
    "*Adversarial Examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA_MNIST\n",
    "x_test_adv = x_test_JSMA_MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4a5f",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the classifier on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee462839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 11.35%\n"
     ]
    }
   ],
   "source": [
    "predictions_adv = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy_adv * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6a1c",
   "metadata": {},
   "source": [
    "Step 5: Split the adversarial test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214595e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_adv_indexes=[]\n",
    "fp_adv_indexes=[]\n",
    "x_test_adv_tp=[]\n",
    "y_test_adv_tp=[]\n",
    "x_test_adv_fp=[]\n",
    "y_test_adv_fp=[]\n",
    "\n",
    "for k in range(len(predictions_adv)):\n",
    "    if(np.argmax(predictions_adv, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_adv_indexes.append(k)\n",
    "    else:\n",
    "        fp_adv_indexes.append(k)\n",
    "\n",
    "for k in tp_adv_indexes:\n",
    "    x_test_adv_tp.append(x_test_adv[k])\n",
    "    y_test_adv_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_adv_indexes:\n",
    "    x_test_adv_fp.append(x_test_adv[k])\n",
    "    y_test_adv_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_adv_tp = np.array(x_test_adv_tp)\n",
    "x_test_adv_fp = np.array(x_test_adv_fp)\n",
    "\n",
    "print('Adversarial TP: {:}'.format(len(x_test_adv_tp)))\n",
    "print('Adversarial FP: {:}'.format(len(x_test_adv_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3b44",
   "metadata": {},
   "source": [
    "Optional step: Plot benign samples and their adversarial counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb593b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "num = 3\n",
    "\n",
    "for i in range(num):\n",
    "    ax = plt.subplot(4, num, i + 1)\n",
    "    plt.imshow(x_test_cln[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(y_test_cln,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, num, i + num + 1)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(predictions_adv,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800d61",
   "metadata": {},
   "source": [
    "## **Section 2 - Defence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d97aeb",
   "metadata": {},
   "source": [
    "### **PixelDefend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f8fb1",
   "metadata": {},
   "source": [
    "Step 1: Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737293cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=16, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448e021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test_adv = x_test_adv.astype(np.float32)\n",
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a341d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1,1,1)\n",
    "plt.imshow(x_test_adv_pd[4], cmap='gray')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pd = defence(x_test_cln*255)[0] / 255\n",
    "x_test_cln_tp_pd = defence(x_test_cln_tp * 255)[0] / 255\n",
    "x_test_cln_fp_pd = defence(x_test_cln_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pd = defence(x_test_adv*255)[0] / 255\n",
    "x_test_adv_tp_pd = defence(x_test_adv_tp * 255)[0] / 255\n",
    "x_test_adv_fp_pd = defence(x_test_adv_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f6b8",
   "metadata": {},
   "source": [
    "Step 2: Evaluate the classifier on all 4 sets of data after PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab56c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_cln_pd = classifier.predict(x_test_cln_pd)\n",
    "accuracy_cln_pd = np.sum(np.argmax(predictions_cln_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Effect of PixelDefend on entire clean test set: {:.2f}%\".format((accuracy_cln_pd - accuracy_cln) * 100))\n",
    " \n",
    "predictions_cln_tp_pd = classifier.predict(x_test_cln_tp_pd)\n",
    "accuracy_cln_tp_pd = np.sum(np.argmax(predictions_cln_tp_pd, axis=1) == np.argmax(y_test_cln_tp, axis=1)) / len(y_test_cln_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive clean test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_cln_tp_pd) * 100))\n",
    "\n",
    "predictions_cln_fp_pd = classifier.predict(x_test_cln_fp_pd)\n",
    "accuracy_cln_fp_pd = np.sum(np.argmax(predictions_cln_fp_pd, axis=1) == np.argmax(y_test_cln_fp, axis=1)) / len(y_test_cln_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_fp_pd * 100))\n",
    "\n",
    "predictions_adv_pd = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv_pd = np.sum(np.argmax(predictions_adv_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"\\nEffect of PixelDefend on entire adversarial test set: {:.2f}%\".format((accuracy_adv_pd-accuracy_adv) * 100))\n",
    "\n",
    "predictions_adv_tp_pd = classifier.predict(x_test_adv_tp_pd)\n",
    "accuracy_adv_tp_pd = np.sum(np.argmax(predictions_adv_tp_pd, axis=1) == np.argmax(y_test_adv_tp, axis=1)) / len(y_test_adv_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive adversarial test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_adv_tp_pd) * 100))\n",
    "\n",
    "predictions_adv_fp_pd = classifier.predict(x_test_adv_fp_pd)\n",
    "accuracy_adv_fp_pd = np.sum(np.argmax(predictions_adv_fp_pd, axis=1) == np.argmax(y_test_adv_fp, axis=1)) / len(y_test_adv_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_fp_pd * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8236f2",
   "metadata": {},
   "source": [
    "Optional step: Plot all data pre- and post-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee10bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot images\n",
    "predictions_cln_tp = classifier.predict(x_test_cln_tp)\n",
    "predictions_cln_fp = classifier.predict(x_test_cln_fp)\n",
    "predictions_adv_tp = classifier.predict(x_test_adv_tp)\n",
    "predictions_adv_fp = classifier.predict(x_test_adv_fp)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#Plot clean true positives\n",
    "ax = plt.subplot(4, 2, 2*0+1)\n",
    "plt.imshow(x_test_cln_tp[0], cmap='gray')\n",
    "ax.set_title('Clean TP: {:}'.format(np.argmax(predictions_cln_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*0+2)\n",
    "plt.imshow(x_test_cln_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean TP after PixelDefend: {:}'.format(np.argmax(predictions_cln_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot clean false positives\n",
    "ax = plt.subplot(4, 2, 2*1+1)\n",
    "plt.imshow(x_test_cln_fp[0], cmap='gray')\n",
    "ax.set_title('Clean FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]), fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*1+2)\n",
    "plt.imshow(x_test_cln_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp_pd,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial true positives\n",
    "ax = plt.subplot(4, 2, 2*2+1)\n",
    "plt.imshow(x_test_adv_tp[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP: {:}'.format(np.argmax(predictions_adv_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*2+2)\n",
    "plt.imshow(x_test_adv_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP after PixelDefend: {:}'.format(np.argmax(predictions_adv_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial false positivies\n",
    "ax = plt.subplot(4, 2, 2*3+1)\n",
    "plt.imshow(x_test_adv_fp[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*3+2)\n",
    "plt.imshow(x_test_adv_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp_pd,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21800af9",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c630",
   "metadata": {},
   "source": [
    "Optional step: Compare the performance of TotalVarMin against the adversary over a range of eps values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# accuracy_original = []\n",
    "# accuracy_robust = []\n",
    "\n",
    "# adv_crafter = FastGradientMethod(classifier)\n",
    "# adv_crafter_robust = FastGradientMethod(robust_classifier)\n",
    "\n",
    "# for eps in eps_range:\n",
    "#     adv_crafter.set_params(**{'eps': eps})\n",
    "#     adv_crafter_robust.set_params(**{'eps': eps})\n",
    "#     x_test_adv = adv_crafter.generate(x_test[:100])\n",
    "#     x_test_adv_robust = adv_crafter_robust.generate(x_test[:100])\n",
    "    \n",
    "#     predictions_original = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "#     accuracy_original += [np.sum(predictions_original == np.argmax(y_test[:100], axis=1))]\n",
    "    \n",
    "#     predictions_robust = np.argmax(robust_classifier.predict(x_test_adv_robust), axis=1)\n",
    "#     accuracy_robust += [np.sum(predictions_robust == np.argmax(y_test[:100], axis=1))]\n",
    "\n",
    "# eps_range = eps_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cbbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_original), 'b--', label='Original classifier')\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_robust), 'r--', label='Robust classifier')\n",
    "\n",
    "# legend = ax.legend(loc='upper right', shadow=True, fontsize='large')\n",
    "# #legend.get_frame().set_facecolor('#00FFCC')\n",
    "\n",
    "# plt.xlabel('Attack strength (eps)')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412463e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f44eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pixel_logits = self.layers(x)\n",
    "        return pixel_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(28*28, 28*28*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 64, 1, 28, 28)\n",
    "\n",
    "        return logit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43826dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "model.load_state_dict(torch.load('/home/cyber/miniconda3/envs/tf-gpu/PixelCNN-Pytorch-master/Models/Model_Checkpoint_Last.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eae160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb76fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c6650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(28*28, 28*28*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 64, 1, 28, 28)\n",
    "\n",
    "        return logit_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b63c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5720a0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define and train the network\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(input=model(images), target=torch.squeeze(images).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, N_EPOCHS, loss.data.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256084fa",
   "metadata": {},
   "source": [
    "**This notebook evalutes the effectiveness of PixelDefend against adversarial attacks on the MARVEL dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdf42",
   "metadata": {},
   "source": [
    "## **Section 0 - Setting Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbf7b5",
   "metadata": {},
   "source": [
    "### **Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d608e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, Layer\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from art import config\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, ProjectedGradientDescent, SaliencyMapMethod, CarliniL2Method, NewtonFool, BasicIterativeMethod\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier, PyTorchClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0f0b",
   "metadata": {},
   "source": [
    "### **Disable eager execution to enable adversarial crafting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87648805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58648ad",
   "metadata": {},
   "source": [
    "### **Load MARVEL dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838b4da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0d2cea95ea70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m#     print(filename)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mload_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test_cln = []\n",
    "y_test_cln = [] \n",
    "min_pixel_value = 0\n",
    "max_pixel_value = 1\n",
    "\n",
    "def marvel_class(filename):\n",
    "    switcher={\n",
    "        'HeavyLoadCarrier': [1,0,0,0,0,0,0,0,0],\n",
    "        'CombatVessel': [0,1,0,0,0,0,0,0,0],\n",
    "        'ContainerShip': [0,0,1,0,0,0,0,0,0],\n",
    "        'PassengersShip': [0,0,0,1,0,0,0,0,0],\n",
    "        'Ro-roCargo': [0,0,0,0,1,0,0,0,0],\n",
    "        'Tanker': [0,0,0,0,0,1,0,0,0],\n",
    "        'Tug': [0,0,0,0,0,0,1,0,0],\n",
    "        'SupplyVessel': [0,0,0,0,0,0,0,1,0],\n",
    "        'Yacht': [0,0,0,0,0,0,0,0,1]\n",
    "    }\n",
    "    return switcher.get(filename)\n",
    "\n",
    "def load_training_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/train_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_train_cln.append(img/255)\n",
    "            y_train_cln.append(marvel_class(filename))\n",
    "            i = i+1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_train_cln, y_train_cln\n",
    "\n",
    "def load_test_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/test_9/\"+filename\n",
    "    i = 0\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_test_cln.append(img/255)\n",
    "            y_test_cln.append(marvel_class(filename))\n",
    "            i = i + 1\n",
    "        if i == 100:\n",
    "            break\n",
    "    return x_test_cln, y_test_cln\n",
    "\n",
    "# for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/train_9\"):\n",
    "#     load_training_data(filename)\n",
    "#     print(filename)\n",
    "\n",
    "for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"):\n",
    "    load_test_data(filename)\n",
    "    print(filename)\n",
    "    \n",
    "#load_training_data(\"/home/cyber/Desktop/Adrian/marvel_data/test_9/CombatVessel\")\n",
    "\n",
    "x_test_cln = np.array(x_test_cln, dtype=np.float32)\n",
    "# x_train_cln = np.array(x_test_cln, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c4a58",
   "metadata": {},
   "source": [
    "### **Create MARVEL classifier model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941dfa3",
   "metadata": {},
   "source": [
    "*Load MARVEL pre-trained model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2237a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/cyber/Desktop/Adrian/Xception-10-0.74.hdf5\"\n",
    "model = load_model(model_path, custom_objects={'RAdam': RAdam}, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2df92",
   "metadata": {},
   "source": [
    "*Create ART classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4d04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bf8e9",
   "metadata": {},
   "source": [
    "### Create PixelCNN for PixelDefend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693efaf7",
   "metadata": {},
   "source": [
    "#### Load PyTorch prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2225b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, utils, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79652df9",
   "metadata": {},
   "source": [
    "#### Create PixelCNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8523f",
   "metadata": {},
   "source": [
    "*Define PixelCNN architecture*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "920063af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCNN(nn.Conv2d):\n",
    "\t\"\"\"\n",
    "\tImplementation of Masked CNN Class as explained in A Oord et. al. \n",
    "\tTaken from https://github.com/jzbontar/pixelcnn-pytorch\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, mask_type, *args, **kwargs):\n",
    "\t\tself.mask_type = mask_type\n",
    "\t\tassert mask_type in ['A', 'B'], \"Unknown Mask Type\"\n",
    "\t\tsuper(MaskedCNN, self).__init__(*args, **kwargs)\n",
    "\t\tself.register_buffer('mask', self.weight.data.clone())\n",
    "\n",
    "\t\t_, depth, height, width = self.weight.size()\n",
    "\t\tself.mask.fill_(1)\n",
    "\t\tif mask_type =='A':\n",
    "\t\t\tself.mask[:,:,height//2,width//2:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\t\telse:\n",
    "\t\t\tself.mask[:,:,height//2,width//2+1:] = 0\n",
    "\t\t\tself.mask[:,:,height//2+1:,:] = 0\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tself.weight.data*=self.mask\n",
    "\t\treturn super(MaskedCNN, self).forward(x)\n",
    "    \n",
    "class PixelCNN(nn.Module):\n",
    "\t\"\"\"\n",
    "\tNetwork of PixelCNN as described in A Oord et. al. \n",
    "\t\"\"\"\n",
    "\tdef __init__(self, no_layers=8, kernel = 7, channels=64, device=None):\n",
    "\t\tsuper(PixelCNN, self).__init__()\n",
    "\t\tself.no_layers = no_layers\n",
    "\t\tself.kernel = kernel\n",
    "\t\tself.channels = channels\n",
    "\t\tself.layers = {}\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.Conv2d_1 = MaskedCNN('A',1,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_1 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_1= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_2 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_2 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_2= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_3 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_3 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_3= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_4 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_4 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_4= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_5 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_5 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_5= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_6 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_6 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_6= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_7 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_7 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_7= nn.ReLU(True)\n",
    "\n",
    "\t\tself.Conv2d_8 = MaskedCNN('B',channels,channels, kernel, 1, kernel//2, bias=False)\n",
    "\t\tself.BatchNorm2d_8 = nn.BatchNorm2d(channels)\n",
    "\t\tself.ReLU_8= nn.ReLU(True)\n",
    "\n",
    "\t\tself.out = nn.Conv2d(channels, 256, 1)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.Conv2d_1(x)\n",
    "\t\tx = self.BatchNorm2d_1(x)\n",
    "\t\tx = self.ReLU_1(x)\n",
    "\n",
    "\t\tx = self.Conv2d_2(x)\n",
    "\t\tx = self.BatchNorm2d_2(x)\n",
    "\t\tx = self.ReLU_2(x)\n",
    "\n",
    "\t\tx = self.Conv2d_3(x)\n",
    "\t\tx = self.BatchNorm2d_3(x)\n",
    "\t\tx = self.ReLU_3(x)\n",
    "\n",
    "\t\tx = self.Conv2d_4(x)\n",
    "\t\tx = self.BatchNorm2d_4(x)\n",
    "\t\tx = self.ReLU_4(x)\n",
    "\n",
    "\t\tx = self.Conv2d_5(x)\n",
    "\t\tx = self.BatchNorm2d_5(x)\n",
    "\t\tx = self.ReLU_5(x)\n",
    "\n",
    "\t\tx = self.Conv2d_6(x)\n",
    "\t\tx = self.BatchNorm2d_6(x)\n",
    "\t\tx = self.ReLU_6(x)\n",
    "\n",
    "\t\tx = self.Conv2d_7(x)\n",
    "\t\tx = self.BatchNorm2d_7(x)\n",
    "\t\tx = self.ReLU_7(x)\n",
    "\n",
    "\t\tx = self.Conv2d_8(x)\n",
    "\t\tx = self.BatchNorm2d_8(x)\n",
    "\t\tx = self.ReLU_8(x)\n",
    "\n",
    "\t\treturn self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb906d2",
   "metadata": {},
   "source": [
    "*Load the weights of a MARVEL pre-trained PixelCNN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9fb6819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PixelCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6e685f638a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPixelCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# model.load_state_dict(torch.load('path.pt'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PixelCNN' is not defined"
     ]
    }
   ],
   "source": [
    "model = PixelCNN()\n",
    "# model.load_state_dict(torch.load('path.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a427c",
   "metadata": {},
   "source": [
    "*Create PixelCNN classifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6291801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "pixelcnn = PyTorchClassifier(\n",
    "    model=model, loss=loss_fn, optimizer=optimizer, input_shape=(28, 28, 1), nb_classes=10, clip_values=(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb76e79",
   "metadata": {},
   "source": [
    "#### Test PixelDefend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb3e24",
   "metadata": {},
   "source": [
    "*Get adversarial examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86c2ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA_MARVEL\n",
    "x_test_adv = x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e374c",
   "metadata": {},
   "source": [
    "*Pre-process input for PixelDefend*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cf55f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "adv = np.transpose(x_test_adv, (0, -1, 1, 2))\n",
    "print(x_test_adv.shape)\n",
    "print(adv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c008688",
   "metadata": {},
   "source": [
    "*Transform input*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acf3fab1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=5, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39f6c904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PixelDefend:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 100\n",
    "x_test_adv_pd = defence(adv[0:num])[0] \n",
    "# x_test_adv_pd = defence(adv[0:num]*255)[0] / 255 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a671c",
   "metadata": {},
   "source": [
    "*Post-process output for classification and plotting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69babf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "x_test_adv_pd = np.transpose(x_test_adv_pd, (0, 2, -1, 1))\n",
    "print(x_test_adv_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56e3e9c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-300d39c62b0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredictions_adv_pd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_adv_pd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maccuracy_adv_pd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions_adv_pd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_cln\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy on adversarial test examples after PixelDefend: {}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_adv_pd\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_adv_pd = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv_pd = np.sum(np.argmax(predictions_adv_pd, axis=1) == np.argmax(y_test_cln[:num], axis=1)) / num\n",
    "\n",
    "print(\"Accuracy on adversarial test examples after PixelDefend: {}%\".format(accuracy_adv_pd * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39ab4b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(4,5,i+1)\n",
    "    plt.imshow(x_test_adv_pd[i], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4,5,i+11)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc077ab",
   "metadata": {},
   "source": [
    "## **Section 1 - Attack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9dd4",
   "metadata": {},
   "source": [
    "Step 1: Evaluate the classifier on the clean test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln = classifier.predict(x_test_cln)\n",
    "accuracy_cln = np.sum(np.argmax(predictions_cln, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on clean test examples: {}%\".format(accuracy_cln * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac38ca",
   "metadata": {},
   "source": [
    "Step 2: Split the clean test set into its true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp_cln_indexes=[]\n",
    "fp_cln_indexes=[]\n",
    "x_test_cln_tp=[]\n",
    "y_test_cln_tp=[]\n",
    "x_test_cln_fp=[]\n",
    "y_test_cln_fp=[]\n",
    "\n",
    "for k in range(len(predictions_cln)):\n",
    "    if(np.argmax(predictions_cln, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_cln_indexes.append(k)\n",
    "    else:\n",
    "        fp_cln_indexes.append(k)\n",
    "\n",
    "for k in tp_cln_indexes:\n",
    "    x_test_cln_tp.append(x_test_cln[k])\n",
    "    y_test_cln_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_cln_indexes:\n",
    "    x_test_cln_fp.append(x_test_cln[k])\n",
    "    y_test_cln_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_cln_tp = np.array(x_test_cln_tp)\n",
    "x_test_cln_fp = np.array(x_test_cln_fp)\n",
    "\n",
    "print('Number of clean true positives: {:}'.format(len(x_test_cln_tp)))\n",
    "print('Number of clean false positives: {:}'.format(len(x_test_cln_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b91dd",
   "metadata": {},
   "source": [
    "Step 3: Craft adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e07528",
   "metadata": {},
   "source": [
    "*Craft Jacobian-based Saliency Map Attack (JSMA) attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ffc57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = SaliencyMapMethod(classifier=classifier, theta = 0.1, gamma=0.3, verbose=True)\n",
    "# x_test_JSMA_MARVEL = attack.generate(x_test_cln)\n",
    "# %store x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d730b68",
   "metadata": {},
   "source": [
    "*Craft Basic Iterative Method (BMI) attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3987b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# attack = BasicIterativeMethod(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_BIM_MARVEL = attack.generate(x_test_cln)\n",
    "# %store x_test_BIM_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76558c01",
   "metadata": {},
   "source": [
    "*Craft Projected Gradient Descent (PGD) attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64259af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = ProjectedGradientDescent(classifier, eps=0.1, eps_step=0.01, max_iter=30)\n",
    "# x_test_PGD_MARVEL = attack.generate(x_test_cln)\n",
    "# %store x_test_PGD_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f431",
   "metadata": {},
   "source": [
    "*Craft NewtonFool attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = NewtonFool(classifier=classifier, eta=0.005, max_iter=25, verbose=True)\n",
    "# x_test_Newton_MARVEL = attack.generate(x_test_cln)\n",
    "# %store x_test_Newton_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a5111d",
   "metadata": {},
   "source": [
    "*Craft DeepFool attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca35c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack = DeepFool(classifier=classifier, epsilon=1e-06/255, max_iter=50)\n",
    "# x_test_Deep_MARVEL = attack.generate(x_test_cln)\n",
    "# %store x_test_Deep_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f9891",
   "metadata": {},
   "source": [
    "*Alternatively, load existing adversarial attacks*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d6ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_name = 'JSMA'\n",
    "%store -r x_test_JSMA_MARVEL\n",
    "x_test_adv = x_test_JSMA_MARVEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4a5f",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the classifier on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee462839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 67.80000000000001%\n"
     ]
    }
   ],
   "source": [
    "predictions_adv = classifier.predict(x_test_adv)\n",
    "accuracy_adv = np.sum(np.argmax(predictions_adv, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy_adv * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6a1c",
   "metadata": {},
   "source": [
    "Step 5: Split the adversarial test set into its true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "214595e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial TP: 6780\n",
      "Adversarial FP: 3220\n"
     ]
    }
   ],
   "source": [
    "tp_adv_indexes=[]\n",
    "fp_adv_indexes=[]\n",
    "x_test_adv_tp=[]\n",
    "y_test_adv_tp=[]\n",
    "x_test_adv_fp=[]\n",
    "y_test_adv_fp=[]\n",
    "\n",
    "for k in range(len(predictions_adv)):\n",
    "    if(np.argmax(predictions_adv, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_adv_indexes.append(k)\n",
    "    else:\n",
    "        fp_adv_indexes.append(k)\n",
    "\n",
    "for k in tp_adv_indexes:\n",
    "    x_test_adv_tp.append(x_test_adv[k])\n",
    "    y_test_adv_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_adv_indexes:\n",
    "    x_test_adv_fp.append(x_test_adv[k])\n",
    "    y_test_adv_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_adv_tp = np.array(x_test_adv_tp)\n",
    "x_test_adv_fp = np.array(x_test_adv_fp)\n",
    "\n",
    "print('Adversarial TP: {:}'.format(len(x_test_adv_tp)))\n",
    "print('Adversarial FP: {:}'.format(len(x_test_adv_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3b44",
   "metadata": {},
   "source": [
    "Step 6: Plot clean test examples and their adversarial counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fb593b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(x_test_cln[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(y_test_cln,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(predictions_adv,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800d61",
   "metadata": {},
   "source": [
    "## **Section 2 - Defence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d97aeb",
   "metadata": {},
   "source": [
    "### **PixelDefend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f8fb1",
   "metadata": {},
   "source": [
    "Step 1: Transform input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147774d2",
   "metadata": {},
   "source": [
    "*Pre-process input for PixelDefend*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f65345",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pre = np.transpose(x_test_cln, (0, -1, 1, 2))\n",
    "x_test_cln_tp_pre = np.transpose(x_test_cln_tp, (0, -1, 1, 2))\n",
    "x_test_cln_fp_pre = np.transpose(x_test_cln_fp, (0, -1, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072deb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pre = np.transpose(x_test_adv, (0, -1, 1, 2))\n",
    "x_test_adv_tp_pre = np.transpose(x_test_adv_tp, (0, -1, 1, 2))\n",
    "x_test_adv_fp_pre = np.transpose(x_test_adv_fp, (0, -1, 1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6508a6",
   "metadata": {},
   "source": [
    "*Transform input*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737293cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "defence = PixelDefend(eps=16, pixel_cnn=pixelcnn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pre_pd = defence(x_test_cln_pre * 255)[0] / 255\n",
    "x_test_cln_tp_pre_pd = defence(x_test_cln_tp_pre * 255)[0] / 255\n",
    "x_test_cln_fp_pre_pd = defence(x_test_cln_fp_pre * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pre_pd = defence(x_test_adv_pre * 255)[0] / 255\n",
    "x_test_adv_tp_pre_pd = defence(x_test_adv_tp_pre * 255)[0] / 255\n",
    "x_test_adv_fp_pre_pd = defence(x_test_adv_fp_pre * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3bd636",
   "metadata": {},
   "source": [
    "*Post-process output for classification and plotting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_cln_pd = np.transpose(x_test_cln_pre_pd, (0, 2, -1, 1))\n",
    "x_test_cln_tp_pd = np.transpose(x_test_cln_tp_pre_pd, (0, 2, -1, 1))\n",
    "x_test_cln_fp_pd = np.transpose(x_test_cln_fp_pre_pd, (0, 2, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_adv_pd = np.transpose(x_test_adv_pre_pd, (0, 2, -1, 1))\n",
    "x_test_adv_tp_pd = np.transpose(x_test_adv_tp_pre_pd, (0, 2, -1, 1))\n",
    "x_test_adv_fp_pd = np.transpose(x_test_adv_fp_pre_pd, (0, 2, -1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f6b8",
   "metadata": {},
   "source": [
    "Step 2: Evaluate the classifier on all 4 sets of data after PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab56c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions_cln_pd = classifier.predict(x_test_cln_pd)\n",
    "accuracy_cln_pd = np.sum(np.argmax(predictions_cln_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Effect of PixelDefend on entire clean test set: {:.2f}%\".format((accuracy_cln_pd - accuracy_cln) * 100))\n",
    " \n",
    "predictions_cln_tp_pd = classifier.predict(x_test_cln_tp_pd)\n",
    "accuracy_cln_tp_pd = np.sum(np.argmax(predictions_cln_tp_pd, axis=1) == np.argmax(y_test_cln_tp, axis=1)) / len(y_test_cln_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive clean test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_cln_tp_pd) * 100))\n",
    "\n",
    "predictions_cln_fp_pd = classifier.predict(x_test_cln_fp_pd)\n",
    "accuracy_cln_fp_pd = np.sum(np.argmax(predictions_cln_fp_pd, axis=1) == np.argmax(y_test_cln_fp, axis=1)) / len(y_test_cln_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive clean test examples after PixelDefend: {:.2f}%\".format(accuracy_cln_fp_pd * 100))\n",
    "\n",
    "predictions_adv_pd = classifier.predict(x_test_adv_pd)\n",
    "accuracy_adv_pd = np.sum(np.argmax(predictions_adv_pd, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"\\nEffect of PixelDefend on entire adversarial test set: {:.2f}%\".format((accuracy_adv_pd-accuracy_adv) * 100))\n",
    "\n",
    "predictions_adv_tp_pd = classifier.predict(x_test_adv_tp_pd)\n",
    "accuracy_adv_tp_pd = np.sum(np.argmax(predictions_adv_tp_pd, axis=1) == np.argmax(y_test_adv_tp, axis=1)) / len(y_test_adv_tp)\n",
    "\n",
    "# print(\"\\nAccuracy on true positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_tp_pd * 100))\n",
    "print(\"\\nAccuracy drop on true positive adversarial test examples after PixelDefend: {:.2f}%\".format((1 - accuracy_adv_tp_pd) * 100))\n",
    "\n",
    "predictions_adv_fp_pd = classifier.predict(x_test_adv_fp_pd)\n",
    "accuracy_adv_fp_pd = np.sum(np.argmax(predictions_adv_fp_pd, axis=1) == np.argmax(y_test_adv_fp, axis=1)) / len(y_test_adv_fp)\n",
    "\n",
    "print(\"\\nAccuracy increase on false positive adversarial test examples after PixelDefend: {:.2f}%\".format(accuracy_adv_fp_pd * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8236f2",
   "metadata": {},
   "source": [
    "Step 3: Plot all data pre- and post-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee10bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot images\n",
    "predictions_cln_tp = classifier.predict(x_test_cln_tp)\n",
    "predictions_cln_fp = classifier.predict(x_test_cln_fp)\n",
    "predictions_adv_tp = classifier.predict(x_test_adv_tp)\n",
    "predictions_adv_fp = classifier.predict(x_test_adv_fp)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "#Plot clean true positives\n",
    "ax = plt.subplot(4, 2, 2*0+1)\n",
    "plt.imshow(x_test_cln_tp[0], cmap='gray')\n",
    "ax.set_title('Clean TP: {:}'.format(np.argmax(predictions_cln_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*0+2)\n",
    "plt.imshow(x_test_cln_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean TP after PixelDefend: {:}'.format(np.argmax(predictions_cln_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot clean false positives\n",
    "ax = plt.subplot(4, 2, 2*1+1)\n",
    "plt.imshow(x_test_cln_fp[0], cmap='gray')\n",
    "ax.set_title('Clean FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]), fontsize=20)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*1+2)\n",
    "plt.imshow(x_test_cln_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Clean FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_cln_fp_pd,axis=1)[0], np.argmax(y_test_cln_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial true positives\n",
    "ax = plt.subplot(4, 2, 2*2+1)\n",
    "plt.imshow(x_test_adv_tp[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP: {:}'.format(np.argmax(predictions_adv_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*2+2)\n",
    "plt.imshow(x_test_adv_tp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial TP after PixelDefend: {:}'.format(np.argmax(predictions_adv_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "#Plot adversarial false positivies\n",
    "ax = plt.subplot(4, 2, 2*3+1)\n",
    "plt.imshow(x_test_adv_fp[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2*3+2)\n",
    "plt.imshow(x_test_adv_fp_pd[0], cmap='gray')\n",
    "ax.set_title('Adversarial FP after PixelDefend: {:}\\nTrue class: {:}'.format(np.argmax(predictions_adv_fp_pd,axis=1)[0], np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "image_path = '/home/cyber/Desktop/Adrian/Plots/PixelDefend_MARVEL_{:}.png'.format(attack_name)\n",
    "plt.savefig(image_path, dpi=500, transparent=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21800af9",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c630",
   "metadata": {},
   "source": [
    "Optional step: Compare the performance of PixelDefend against the adversary over a range of eps values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# accuracy_original = []\n",
    "# accuracy_robust = []\n",
    "\n",
    "# attack = FastGradientMethod(classifier)\n",
    "# attack_robust = FastGradientMethod(robust_classifier)\n",
    "\n",
    "# for eps in eps_range:\n",
    "#     attack.set_params(**{'eps': eps})\n",
    "#     attack_robust.set_params(**{'eps': eps})\n",
    "#     x_test_adv = attack.generate(x_test[:100])\n",
    "#     x_test_adv_robust = attack_robust.generate(x_test[:100])\n",
    "    \n",
    "#     predictions_original = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "#     accuracy_original += [np.sum(predictions_original == np.argmax(y_test[:100], axis=1))]\n",
    "    \n",
    "#     predictions_robust = np.argmax(robust_classifier.predict(x_test_adv_robust), axis=1)\n",
    "#     accuracy_robust += [np.sum(predictions_robust == np.argmax(y_test[:100], axis=1))]\n",
    "\n",
    "# eps_range = eps_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cbbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_original), 'b--', label='Original classifier')\n",
    "# ax.plot(np.array(eps_range), np.array(accuracy_robust), 'r--', label='Robust classifier')\n",
    "\n",
    "# legend = ax.legend(loc='upper right', shadow=True, fontsize='large')\n",
    "# #legend.get_frame().set_facecolor('#00FFCC')\n",
    "\n",
    "# plt.xlabel('Attack strength (eps)')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09a11c",
   "metadata": {},
   "source": [
    "## PixelCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3078815",
   "metadata": {},
   "source": [
    "### Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(width, height, starting_point):\n",
    "    row_grid, col_grid = np.meshgrid(np.arange(width), np.arange(height), indexing='ij')\n",
    "    mask = np.logical_or(\n",
    "        row_grid < starting_point[0],\n",
    "        np.logical_and(row_grid == starting_point[0], col_grid <= starting_point[1]))\n",
    "    return mask\n",
    "\n",
    "def conv_mask(width, height, include_center=False):\n",
    "    return 1.0 * causal_mask(width, height, starting_point=(width//2, height//2 + include_center - 1))\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        _, n_channels, width, height = self.weight.size()\n",
    "\n",
    "        mask = conv_mask(width, height, include_center=mask_type=='B')\n",
    "        self.register_buffer('mask', torch.from_numpy(mask).float())\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    n_channels = 4\n",
    "    kernel_size = 7\n",
    "    padding = 3\n",
    "    n_pixels_out = 2 # binary 0/1 pixels\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            MaskedConv2d('A', in_channels=1, out_channels=self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            MaskedConv2d('B', self.n_channels, self.n_channels, kernel_size=self.kernel_size, padding=self.padding, bias=False), nn.BatchNorm2d(self.n_channels), nn.ReLU(True),\n",
    "            nn.Conv2d(in_channels=self.n_channels, out_channels=self.n_pixels_out, kernel_size=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(28*28, 28*28*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        logit_output = self.fc(x)\n",
    "        logit_output = logit_output.view(-1, 64, 1, 28, 28)\n",
    "\n",
    "        return logit_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25319c6a",
   "metadata": {},
   "source": [
    "### Attempt 2: PixelCNN by Jzbontar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b17c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('torch.cuda.is_available():', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ee6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from torch import nn, optim, cuda, backends\n",
    "from torch.utils import data\n",
    "backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
    "        assert mask_type in {'A', 'B'}\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask.fill_(1)\n",
    "        self.mask[:, :, kH // 2, kW // 2 + (mask_type == 'B'):] = 0\n",
    "        self.mask[:, :, kH // 2 + 1:] = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv2d, self).forward(x)\n",
    "\n",
    "fm = 64\n",
    "model = nn.Sequential(\n",
    "    MaskedConv2d('A', 1,  fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    MaskedConv2d('B', fm, fm, 7, 1, 3, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n",
    "    nn.Conv2d(fm, 256, 1))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "tr = data.DataLoader(datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "                     batch_size=128, shuffle=True, num_workers=1, pin_memory=True)\n",
    "te = data.DataLoader(datasets.MNIST('data', train=False, download=True, transform=transforms.ToTensor()),\n",
    "                     batch_size=128, shuffle=False, num_workers=1, pin_memory=True)\n",
    "sample = torch.Tensor(144, 1, 28, 28).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "# for epoch in range(5):\n",
    "#     # train\n",
    "#     err_tr = []\n",
    "#     cuda.synchronize()\n",
    "#     time_tr = time.time()\n",
    "#     model.train(True)\n",
    "#     for input, _ in tr:\n",
    "#         input = Variable(input.cuda(non_blocking=True))\n",
    "#         target = Variable((input.data[:,0] * 255).long())\n",
    "#         loss = F.cross_entropy(model(input), target)\n",
    "#         err_tr.append(loss.data.item())\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     cuda.synchronize()\n",
    "#     time_tr = time.time() - time_tr\n",
    "\n",
    "#     # compute error on test set\n",
    "#     err_te = []\n",
    "#     cuda.synchronize()\n",
    "#     time_te = time.time()\n",
    "#     model.train(False)\n",
    "#     for input, _ in te:\n",
    "#         input = Variable(input.cuda(non_blocking=True), volatile=True)\n",
    "#         target = Variable((input.data[:,0] * 255).long())\n",
    "#         loss = F.cross_entropy(model(input), target)\n",
    "#         err_te.append(loss.data.item())\n",
    "#     cuda.synchronize()\n",
    "#     time_te = time.time() - time_te\n",
    "\n",
    "#     # sample\n",
    "#     sample.fill_(0)\n",
    "#     model.train(False)\n",
    "#     for i in range(28):\n",
    "#         for j in range(28):\n",
    "#             out = model(Variable(sample, volatile=True))\n",
    "#             probs = F.softmax(out[:, :, i, j]).data\n",
    "#             sample[:, :, i, j] = torch.multinomial(probs, 1).float() / 255.\n",
    "#     utils.save_image(sample, 'sample_{:02d}.png'.format(epoch), nrow=12, padding=0)\n",
    "\n",
    "#     print('epoch={}; nll_tr={:.7f}; nll_te={:.7f}; time_tr={:.1f}s; time_te={:.1f}s'.format(\n",
    "#         epoch, np.mean(err_tr), np.mean(err_te), time_tr, time_te))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64925eb5",
   "metadata": {},
   "source": [
    "### Attempt 3: PixelCNN by Kamenbliznashki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PixelCNN implementation\n",
    "References:\n",
    "    1. van den Oord, Pixel Recurrent Neural Networks 2016a\n",
    "    2. van den Oord, Conditional Image Generation with PixelCNN Decoders, 2016c\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "image_dims=(1,28,28), n_bits=4, n_channels=128, n_out_conv_channels=1024, kernel_size=5, n_res_layers=12, n_cond_classes=10\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Model components\n",
    "# --------------------\n",
    "\n",
    "def pixelcnn_gate(x):\n",
    "    a, b = x.chunk(2,1)\n",
    "    return torch.tanh(a) * torch.sigmoid(b)\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, mask_type=None, mask_n_channels=None, gated=False, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "#         nn.init.constant_(bias, 0.)\n",
    "\n",
    "        # set up mask -- cf PixelRNN paper Figure 2 Right: masks A and B\n",
    "        mask_type = mask_type\n",
    "        mask_n_channels = mask_n_channels\n",
    "        center_row = kernel_size[0] // 2\n",
    "        center_col = kernel_size[1] // 2\n",
    "\n",
    "        mask = torch.ones_like(weight)         # shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "        # mask out 1/ rows below the middle and 2/ center row pixels right of middle\n",
    "        if center_row == 0:                         # case when kernel_size = (1,k) in horizontal stack\n",
    "            mask[:, :, :, center_col+1:] = 0\n",
    "        elif center_col == 0:                       # case when kernel_size = (k,1)\n",
    "            mask[:, :, center_row+1:, :] = 0\n",
    "        else:                                       # case when kernel_size = (k,k)\n",
    "            mask[:, :, center_row+1:, :] = 0\n",
    "            mask[:, :, center_row, center_col+1:] = 0\n",
    "\n",
    "        # mask out center pixel in future channels -- mask A current channel is 0; mask B current channel is 1\n",
    "        for i in range(mask_n_channels):\n",
    "            for j in range(mask_n_channels):\n",
    "                if (mask_type=='a' and i >= j) or (mask_type=='b' and i > j):\n",
    "                    mask[j::mask_n_channels, i::mask_n_channels, center_row, center_col] = 0\n",
    "\n",
    "        # mask out center row (vertical stack in a Gated Residual Layer); cf Conditional image generation with PixelCNN Decoders\n",
    "        if mask_type == 'vstack':\n",
    "            mask[:, :, center_row, :] = 0\n",
    "\n",
    "        if gated:\n",
    "            # pixelcnn gate splits the input in two along the channel dim;\n",
    "            # ensure that both chunks receive the same mask by replicating the first half of the mask over the second\n",
    "            mask = mask.chunk(2,0)[0].repeat(2,1,1,1)\n",
    "\n",
    "        # final mask\n",
    "        register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight.data *= mask\n",
    "        return super().forward(x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = super().__repr__()\n",
    "        return s[:-1] + ', mask_type={}, mask_n_channels={}'.format(mask_type, mask_n_channels) + s[-1]\n",
    "\n",
    "\n",
    "class GatedResidualLayer(nn.Module):\n",
    "    \"\"\" Figure 2 in Conditional image generation with PixelCNN Decoders \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, mask_type, mask_n_channels, n_cond_classes, norm_layer):\n",
    "        super().__init__()\n",
    "        residual = (in_channels==out_channels)\n",
    "        norm_layer = norm_layer\n",
    "\n",
    "        v   = MaskedConv2d(in_channels, 2*out_channels, kernel_size, padding=kernel_size//2,\n",
    "                                mask_type='vstack', mask_n_channels=mask_n_channels, gated=True)\n",
    "        h   = MaskedConv2d(in_channels, 2*out_channels, (1, kernel_size), padding=(0, kernel_size//2),\n",
    "                                mask_type=mask_type, mask_n_channels=mask_n_channels, gated=True)\n",
    "        v2h = MaskedConv2d(2*out_channels, 2*out_channels, kernel_size=1,\n",
    "                                mask_type=mask_type, mask_n_channels=mask_n_channels, gated=True)\n",
    "        h2h = MaskedConv2d(out_channels, out_channels, kernel_size=1,\n",
    "                                mask_type=mask_type, mask_n_channels=mask_n_channels, gated=False)\n",
    "\n",
    "        if n_cond_classes:\n",
    "            proj_h = nn.Linear(n_cond_classes, 2*out_channels)\n",
    "\n",
    "        if norm_layer:\n",
    "            norm_layer_v = nn.BatchNorm2d(out_channels)\n",
    "            norm_layer_h = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x_v, x_h, h=None):\n",
    "        # projection of h if included for conditional generation (cf paper section 2.3 -- added before the pixelcnn_gate)\n",
    "        proj_y = proj_h(h)[:,:,None,None] if h is not None else 0\n",
    "\n",
    "        # vertical stack\n",
    "        x_v_out = v(x_v)\n",
    "        x_v2h = v2h(x_v_out) + proj_y\n",
    "        x_v_out = pixelcnn_gate(x_v_out)\n",
    "\n",
    "        # horizontal stack\n",
    "        x_h_out = h(x_h) + x_v2h + proj_y\n",
    "        x_h_out = pixelcnn_gate(x_h_out)\n",
    "        x_h_out = h2h(x_h_out)\n",
    "\n",
    "        # residual connection\n",
    "        if residual:\n",
    "            x_h_out = x_h_out + x_h\n",
    "\n",
    "        # normalization\n",
    "        if norm_layer:\n",
    "            x_v_out = norm_layer_v(x_v_out)\n",
    "            x_h_out = norm_layer_h(x_h_out)\n",
    "\n",
    "        return x_v_out, x_h_out\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'residual={}, norm_layer={}'.format(residual, norm_layer)\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# PixelCNN\n",
    "# --------------------\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, image_dims, n_bits, n_channels, n_out_conv_channels, kernel_size, n_res_layers, n_cond_classes, norm_layer=True):\n",
    "        super().__init__()\n",
    "        C, H, W = image_dims\n",
    "\n",
    "        input_conv = MaskedConv2d(C, 2*n_channels, kernel_size=7, padding=3, mask_type='a', mask_n_channels=C, gated=True)\n",
    "        res_layers = nn.ModuleList([\n",
    "            GatedResidualLayer(n_channels, n_channels, kernel_size, 'b', C, n_cond_classes, norm_layer)\n",
    "            for _ in range(n_res_layers)])\n",
    "        conv_out1 = MaskedConv2d(n_channels, 2*n_out_conv_channels, kernel_size=1, mask_type='b', mask_n_channels=C, gated=True)\n",
    "        conv_out2 = MaskedConv2d(n_out_conv_channels, 2*n_out_conv_channels, kernel_size=1, mask_type='b', mask_n_channels=C, gated=True)\n",
    "        output = MaskedConv2d(n_out_conv_channels, C * 2**n_bits, kernel_size=1, mask_type='b', mask_n_channels=C)\n",
    "\n",
    "        if n_cond_classes:\n",
    "            proj_h = nn.Linear(n_cond_classes, 2*n_channels)\n",
    "\n",
    "    def forward(self, x, h=None):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        x = pixelcnn_gate(input_conv(x) + (proj_h(h)[:,:,None,None] if h is not None else 0.))\n",
    "        x_v, x_h = x, x\n",
    "\n",
    "        for l in res_layers:\n",
    "            x_v, x_h = l(x_v, x_h)\n",
    "\n",
    "        out = pixelcnn_gate(conv_out1(x_h))\n",
    "        out = pixelcnn_gate(conv_out2(out))\n",
    "        out = output(out)\n",
    "\n",
    "        return out.reshape(B, -1, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ddf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN(image_dims=(1,28,28), n_bits=4, n_channels=128, n_out_conv_channels=1024, kernel_size=5, n_res_layers=12, n_cond_classes=10)\n",
    "model.load_state_dict(torch.load('/home/cyber/miniconda3/envs/tf-gpu/pixel_models-master/results/pixelcnn/2021-05-04_08-44-00/checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ec8c9",
   "metadata": {},
   "source": [
    "### Attempt 4: PixelCNN by pclucas14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f537d2ee",
   "metadata": {},
   "source": [
    "### Train PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "lr = 0.002\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dff8cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/cyber/Desktop/Adrian\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ade13611005439cb13a84f0478e688e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-136a996e8f20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                \u001b[0mtorchvision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                torchvision.transforms.Normalize(\n\u001b[1;32m----> 6\u001b[1;33m                                  (0.1307,), (0.3081,))\n\u001b[0m\u001b[0;32m      7\u001b[0m                              ])),\n\u001b[0;32m      8\u001b[0m   batch_size=batch_size_train, shuffle=True)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m                         \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m                         \u001b[0mmd5\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m                     )\n\u001b[0;32m    160\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mdownload_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[0marchive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0m_urlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[1;34m(url, filename, chunk_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torchvision\\datasets\\utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    459\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST(root=r'/home/cyber/Desktop/Adrian', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95024cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246437e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "  model.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = F.cross_entropy(input=model(data), target=torch.squeeze(data).long())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "      torch.save(model.state_dict(), '/home/cyber/Desktop/Adrian/PixelCNN_Singh_Hrituraj/model.pt')\n",
    "      torch.save(optimizer.state_dict(), '/home/cyber/Desktop/Adrian/PixelCNN_Singh_Hrituraj/optimizer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelCNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d051781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc667a",
   "metadata": {},
   "source": [
    "*Training with classifier fit*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d08c4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cln_pre = np.transpose(x_train_cln, (0, -1, 1, 2))\n",
    "x_train_cln_pre = np.array(x_train_cln_pre, dtype=np.float32)\n",
    "y_train_cln_pre = np.array(y_train_cln, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6b85fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cln_pre = np.expand_dims(y_train_cln_pre, axis=2)\n",
    "y_train_cln_pre = np.expand_dims(y_train_cln_pre, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac401968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_cln_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bbff9a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only batches of spatial targets supported (3D tensors) but got targets of dimension: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-767ff7d3afaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpixelcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_cln_pre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_cln_pre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\art\\estimators\\classification\\classifier.py\u001b[0m in \u001b[0;36mreplacement_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mreplacement_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfunc_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\art\\estimators\\classification\\pytorch.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, nb_epochs, **kwargs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[1;31m# Form the loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_batch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# lgtm [py/call-to-non-callable]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                 \u001b[1;31m# Do training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m-> 1048\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2388\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2389\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2390\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2391\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2392\u001b[0m         \u001b[1;31m# dim == 3 or dim > 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1"
     ]
    }
   ],
   "source": [
    "pixelcnn.fit(x_train_cln_pre, y_train_cln_pre, 64, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d114b",
   "metadata": {},
   "source": [
    "### Pre-process input for PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc25943",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv = torch.from_numpy(x_test_adv)\n",
    "adv = adv.transpose(1,-1)\n",
    "adv = adv.numpy()\n",
    "# adv = adv.to('cuda')\n",
    "print(x_test_adv.shape)\n",
    "print(adv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db800730",
   "metadata": {},
   "source": [
    "### Check if GPU is recognised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e410c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16300606085450404707\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8760374563923678448\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3127299276\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12812551143669546485\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1050 Ti with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18289867501710258441\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93760fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5490c276",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2eaca526b1e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851233a",
   "metadata": {},
   "source": [
    "### Train and save a model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c6929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_cln, y_train_cln, batch_size=64, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a91e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/cyber/dataset_trained_model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256084fa",
   "metadata": {},
   "source": [
    "**This notebook focuses on the effectiveness of PixelDefend against adversarial attacks on the MNIST and MARVEL datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cdf42",
   "metadata": {},
   "source": [
    "## **Section 0 - Setting Up**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dbf7b5",
   "metadata": {},
   "source": [
    "### **Load prerequisites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d608e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Activation, Dropout, Layer\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from art import config\n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, ProjectedGradientDescent, SaliencyMapMethod, CarliniL2Method, NewtonFool, BasicIterativeMethod\n",
    "from art.defences.preprocessor import PixelDefend\n",
    "from art.defences.trainer import AdversarialTrainer\n",
    "from art.estimators.classification import KerasClassifier, TensorFlowV2Classifier\n",
    "from art.utils import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f86a2d2",
   "metadata": {},
   "source": [
    "## Build a PixelCNN for PixelDefend to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b8ab9",
   "metadata": {},
   "source": [
    "### Tensorflow probability method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a677a39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function image_preprocess at 0x7ff0169f8ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function image_preprocess at 0x7ff0169f8ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function image_preprocess at 0x7ff0169f8ee0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:Output tf_op_layer_PixelCNN_1/log_prob/Reshape_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_PixelCNN_1/log_prob/Reshape_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output tf_op_layer_PixelCNN_1/log_prob/Reshape_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to tf_op_layer_PixelCNN_1/log_prob/Reshape_1.\n"
     ]
    }
   ],
   "source": [
    "# Build a small Pixel CNN++ model to train on MNIST.\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "\n",
    "# tf.enable_v2_behavior()\n",
    "\n",
    "# Load MNIST from tensorflow_datasets\n",
    "data = tfds.load('mnist')\n",
    "train_data, test_data = data['train'], data['test']\n",
    "\n",
    "def image_preprocess(x):\n",
    "  x['image'] = tf.cast(x['image'], tf.float32)\n",
    "  return (x['image'],)  # (input, output) of the model\n",
    "\n",
    "batch_size = 16\n",
    "train_it = train_data.map(image_preprocess).batch(batch_size).shuffle(1000)\n",
    "\n",
    "image_shape = (28, 28, 1)\n",
    "# Define a Pixel CNN network\n",
    "dist = tfd.PixelCNN(\n",
    "    image_shape=image_shape,\n",
    "    num_resnet=1,\n",
    "    num_hierarchies=2,\n",
    "    num_filters=32,\n",
    "    num_logistic_mix=5,\n",
    "    dropout_p=.3,\n",
    ")\n",
    "\n",
    "# Define the model input\n",
    "image_input = tfkl.Input(shape=image_shape)\n",
    "\n",
    "# Define the log likelihood for the loss fn\n",
    "log_prob = dist.log_prob(image_input)\n",
    "\n",
    "# Define the model\n",
    "model = tfk.Model(inputs=image_input, outputs=log_prob)\n",
    "model.add_loss(-tf.reduce_mean(log_prob))\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(\n",
    "    optimizer=tfk.optimizers.Adam(.001),\n",
    "    metrics=[])\n",
    "\n",
    "# model.fit(train_it, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5da04f",
   "metadata": {},
   "source": [
    "### Keras Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8cef786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small Pixel CNN++ model to train on MNIST.\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f02ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "n_residual_blocks = 5\n",
    "# The data, split between train and test sets\n",
    "(x, _), (y, _) = keras.datasets.mnist.load_data()\n",
    "x, y = x[:1000], y[:1000]\n",
    "# Concatenate all of the images together\n",
    "data = np.concatenate((x, y), axis=0)\n",
    "# Round all pixel values less than 33% of the max 256 value to 0\n",
    "# anything above this value gets rounded up to 1 so that all values are either\n",
    "# 0 or 1\n",
    "data = np.where(data < (0.33 * 256), 0, 1)\n",
    "data = data.astype(np.float32)\n",
    "\n",
    "data = np.expand_dims(data, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1516b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer is the PixelCNN layer. This layer simply\n",
    "# builds on the 2D convolutional layer, but includes masking.\n",
    "class PixelConvLayer(Layer):\n",
    "    def __init__(self, mask_type, **kwargs):\n",
    "        super(PixelConvLayer, self).__init__()\n",
    "        self.mask_type = mask_type\n",
    "        self.conv = Conv2D(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Build the conv2d layer to initialize kernel variables\n",
    "        self.conv.build(input_shape)\n",
    "        # Use the initialized kernel to create the mask\n",
    "        kernel_shape = self.conv.kernel.get_shape()\n",
    "        self.mask = np.zeros(shape=kernel_shape)\n",
    "        self.mask[: kernel_shape[0] // 2, ...] = 1.0\n",
    "        self.mask[kernel_shape[0] // 2, : kernel_shape[1] // 2, ...] = 1.0\n",
    "        if self.mask_type == \"B\":\n",
    "            self.mask[kernel_shape[0] // 2, kernel_shape[1] // 2, ...] = 1.0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.conv.kernel.assign(self.conv.kernel * self.mask)\n",
    "        return self.conv(inputs)\n",
    "\n",
    "\n",
    "# Next, we build our residual block layer.\n",
    "# This is just a normal residual block, but based on the PixelConvLayer.\n",
    "class ResidualBlock(Layer):\n",
    "    def __init__(self, filters, **kwargs):\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "        self.pixel_conv = PixelConvLayer(\n",
    "            mask_type=\"B\",\n",
    "            filters=filters // 2,\n",
    "            kernel_size=3,\n",
    "            activation=\"relu\",\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.conv2 = Conv2D(\n",
    "            filters=filters, kernel_size=1, activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.pixel_conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return tf.keras.layers.add([inputs, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add11aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "pixel_conv_layer (PixelConvL (None, 28, 28, 128)       6400      \n",
      "_________________________________________________________________\n",
      "residual_block (ResidualBloc (None, 28, 28, 128)       98624     \n",
      "_________________________________________________________________\n",
      "residual_block_1 (ResidualBl (None, 28, 28, 128)       98624     \n",
      "_________________________________________________________________\n",
      "residual_block_2 (ResidualBl (None, 28, 28, 128)       98624     \n",
      "_________________________________________________________________\n",
      "residual_block_3 (ResidualBl (None, 28, 28, 128)       98624     \n",
      "_________________________________________________________________\n",
      "residual_block_4 (ResidualBl (None, 28, 28, 128)       98624     \n",
      "_________________________________________________________________\n",
      "pixel_conv_layer_6 (PixelCon (None, 28, 28, 128)       16512     \n",
      "_________________________________________________________________\n",
      "pixel_conv_layer_7 (PixelCon (None, 28, 28, 128)       16512     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 1)         129       \n",
      "=================================================================\n",
      "Total params: 532,673\n",
      "Trainable params: 532,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "15/15 [==============================] - 14s 524ms/step - loss: 0.6127 - val_loss: 0.2761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab3d921100>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=input_shape)\n",
    "x = PixelConvLayer(\n",
    "    mask_type=\"A\", filters=128, kernel_size=7, activation=\"relu\", padding=\"same\"\n",
    ")(inputs)\n",
    "\n",
    "for _ in range(n_residual_blocks):\n",
    "    x = ResidualBlock(filters=128)(x)\n",
    "\n",
    "for _ in range(2):\n",
    "    x = PixelConvLayer(\n",
    "        mask_type=\"B\",\n",
    "        filters=128,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        activation=\"relu\",\n",
    "        padding=\"valid\",\n",
    "    )(x)\n",
    "\n",
    "out = Conv2D(\n",
    "    filters=1, kernel_size=1, strides=1, activation=\"sigmoid\", padding=\"valid\"\n",
    ")(x)\n",
    "\n",
    "pixel_cnn = keras.Model(inputs, out)\n",
    "adam = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "pixel_cnn.compile(optimizer=adam, loss=\"binary_crossentropy\")\n",
    "\n",
    "# data.shape\n",
    "\n",
    "pixel_cnn.summary()\n",
    "pixel_cnn.fit(\n",
    "    x=data, y=data, batch_size=128, epochs=1, validation_split=0.1, verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfe9f76c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-017e4a8659bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpixel_cnn_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/keras.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, use_logits, channels_first, clip_values, preprocessing_defences, postprocessing_defences, preprocessing, input_layer, output_layer)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type of model not recognized:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     def _initialize_params(\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/estimators/classification/keras.py\u001b[0m in \u001b[0;36m_initialize_params\u001b[0;34m(self, model, use_logits, input_layer, output_layer)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nb_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         logger.debug(\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "pixel_cnn_classifier = KerasClassifier(model=pixel_cnn, use_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41424626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn_classifier = TensorFlowV2Classifier(pixel_cnn, nb_classes=num_classes, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f0f0b",
   "metadata": {},
   "source": [
    "### **Modification: Disabling eager execution to enable adversarial crafting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87648805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58648ad",
   "metadata": {},
   "source": [
    "### **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2aa87dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_cln, y_train_cln), (x_test_cln, y_test_cln), min_pixel_value, max_pixel_value = load_mnist()\n",
    "x_train_cln, y_train_cln = x_train_cln[:1000], y_train_cln[:1000]\n",
    "# x_test_cln, y_test_cln = x_test_cln[:500], y_test_cln[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070c4a58",
   "metadata": {},
   "source": [
    "### **Load / Create classifier model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8ddd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST pre-trained model\n",
    "model = load_model(\"/home/cyber/mnist_trained_model.h5\")\n",
    "\n",
    "#MARVEL pre-trained model\n",
    "# model = load_model(\"/home/cyber/Desktop/Adrian/Xception-10-0.74.hdf5\")\n",
    "\n",
    "classifier = KerasClassifier(model=model, clip_values=(min_pixel_value, max_pixel_value), use_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141f6bd9",
   "metadata": {},
   "source": [
    "*Optional step: Train and save a model for future use*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55cabb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train_cln, y_train_cln, batch_size=64, epochs=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "399ff0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"/home/cyber/dataset_trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc077ab",
   "metadata": {},
   "source": [
    "## **Section 1 - Attack**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec9dd4",
   "metadata": {},
   "source": [
    "Step 1: Evaluate the classifier on benign test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1362d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyber/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 99.11%\n"
     ]
    }
   ],
   "source": [
    "predictions_cln = classifier.predict(x_test_cln)\n",
    "accuracy = np.sum(np.argmax(predictions_cln, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac38ca",
   "metadata": {},
   "source": [
    "Step 2: Split benign test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7909c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign TP: 9911\n",
      "Benign FP: 89\n"
     ]
    }
   ],
   "source": [
    "tp_cln_indexes=[]\n",
    "fp_cln_indexes=[]\n",
    "x_test_cln_tp=[]\n",
    "y_test_cln_tp=[]\n",
    "x_test_cln_fp=[]\n",
    "y_test_cln_fp=[]\n",
    "\n",
    "for k in range(len(predictions_cln)):\n",
    "    if(np.argmax(predictions_cln, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_cln_indexes.append(k)\n",
    "    else:\n",
    "        fp_cln_indexes.append(k)\n",
    "\n",
    "for k in tp_cln_indexes:\n",
    "    x_test_cln_tp.append(x_test_cln[k])\n",
    "    y_test_cln_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_cln_indexes:\n",
    "    x_test_cln_fp.append(x_test_cln[k])\n",
    "    y_test_cln_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_cln_tp = np.array(x_test_cln_tp)\n",
    "x_test_cln_fp = np.array(x_test_cln_fp)\n",
    "\n",
    "print('Benign TP: {:}'.format(len(x_test_cln_tp)))\n",
    "print('Benign FP: {:}'.format(len(x_test_cln_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334b91dd",
   "metadata": {},
   "source": [
    "Step 3: Craft adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d730b68",
   "metadata": {},
   "source": [
    "*Basic Iterative Method (BMI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3987b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adv_crafter = BasicIterativeMethod(classifier, eps=0.3, eps_step=0.1, max_iter=100)\n",
    "x_test_BIM = adv_crafter.generate(x_test_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76558c01",
   "metadata": {},
   "source": [
    "*Projected Gradient Descent (PGD)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64259af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_crafter = ProjectedGradientDescent(classifier, eps=0.3, eps_step=0.1, max_iter=100)\n",
    "x_test_PGD = adv_crafter.generate(x_test_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad2f431",
   "metadata": {},
   "source": [
    "*NewtonFool*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_crafter = NewtonFool(classifier, eta=0.01, max_iter=100, verbose=True)\n",
    "x_test_Newton = adv_crafter.generate(x_test_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e07528",
   "metadata": {},
   "source": [
    "*Jacobian-based Saliency Map Attack (JSMA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffc57cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_crafter = SaliencyMapMethod(classifier, theta = 0.1, gamma=1, verbose=True)\n",
    "x_test_JSMA = adv_crafter.generate(x_test_cln)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f9891",
   "metadata": {},
   "source": [
    "*Adversarial Examples*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d6ff9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r x_test_JSMA\n",
    "x_test_adv = x_test_JSMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f4a5f",
   "metadata": {},
   "source": [
    "Step 4: Evaluate the classifier on the adversarial test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee462839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test examples: 0.19%\n"
     ]
    }
   ],
   "source": [
    "predictions_adv = classifier.predict(x_test_adv)\n",
    "accuracy = np.sum(np.argmax(predictions_adv, axis=1) == np.argmax(y_test_cln, axis=1)) / len(y_test_cln)\n",
    "\n",
    "print(\"Accuracy on adversarial test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6a1c",
   "metadata": {},
   "source": [
    "Step 5: Split the adversarial test examples into true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "214595e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial TP: 19\n",
      "Adversarial FP: 9981\n"
     ]
    }
   ],
   "source": [
    "tp_adv_indexes=[]\n",
    "fp_adv_indexes=[]\n",
    "x_test_adv_tp=[]\n",
    "y_test_adv_tp=[]\n",
    "x_test_adv_fp=[]\n",
    "y_test_adv_fp=[]\n",
    "\n",
    "for k in range(len(predictions_adv)):\n",
    "    if(np.argmax(predictions_adv, axis=1)[k] == np.argmax(y_test_cln, axis=1)[k]):\n",
    "        tp_adv_indexes.append(k)\n",
    "    else:\n",
    "        fp_adv_indexes.append(k)\n",
    "\n",
    "for k in tp_adv_indexes:\n",
    "    x_test_adv_tp.append(x_test_adv[k])\n",
    "    y_test_adv_tp.append(y_test_cln[k])\n",
    "    \n",
    "for k in fp_adv_indexes:\n",
    "    x_test_adv_fp.append(x_test_adv[k])\n",
    "    y_test_adv_fp.append(y_test_cln[k])\n",
    "    \n",
    "x_test_adv_tp = np.array(x_test_adv_tp)\n",
    "x_test_adv_fp = np.array(x_test_adv_fp)\n",
    "\n",
    "print('Adversarial TP: {:}'.format(len(x_test_adv_tp)))\n",
    "print('Adversarial FP: {:}'.format(len(x_test_adv_fp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce3b44",
   "metadata": {},
   "source": [
    "Optional step: Plot benign samples and their adversarial counterparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fb593b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    ax = plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(x_test_cln[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(y_test_cln,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    ax = plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(x_test_adv[i], cmap='gray')\n",
    "    ax.set_title('{:}'.format(np.argmax(predictions_adv,axis=1)[i]))\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a800d61",
   "metadata": {},
   "source": [
    "## **Section 2 - Defence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d97aeb",
   "metadata": {},
   "source": [
    "### **PixelDefend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f8fb1",
   "metadata": {},
   "source": [
    "Step 1: Transform input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "737293cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PixelCNN model must be of type Classifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-50667fac23db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdefence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPixelDefend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_cnn_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_test_cln_tp_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_cln_tp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_test_cln_fp_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_cln_fp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_test_adv_tp_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_adv_tp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/defences/preprocessor/pixel_defend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, clip_values, eps, pixel_cnn, batch_size, apply_fit, apply_predict, verbose)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_cnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/art/defences/preprocessor/pixel_defend.py\u001b[0m in \u001b[0;36m_check_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpixel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNeuralNetworkMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         ):\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PixelCNN model must be of type Classifier.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PixelCNN model must be of type Classifier."
     ]
    }
   ],
   "source": [
    "defence = PixelDefend(pixel_cnn_classifier)\n",
    "\n",
    "x_test_cln_tp_pd = defence(x_test_cln_tp * 255)[0] / 255\n",
    "x_test_cln_fp_pd = defence(x_test_cln_fp * 255)[0] / 255\n",
    "x_test_adv_tp_pd = defence(x_test_adv_tp * 255)[0] / 255\n",
    "x_test_adv_fp_pd = defence(x_test_adv_fp * 255)[0] / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9f6b8",
   "metadata": {},
   "source": [
    "Step 2: Evaluate the classifier on all 4 sets of data after PixelDefend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ab56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln_tp_pd = classifier.predict(x_test_cln_tp_pd)\n",
    "accuracy = np.sum(np.argmax(predictions_cln_tp_pd, axis=1) == np.argmax(y_test_cln_tp_pd, axis=1)) / len(y_test_cln_tp_pd)\n",
    "\n",
    "print(\"Accuracy on true positive benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca752f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln_fp_pd = classifier.predict(x_test_cln_fp_pd)\n",
    "accuracy = np.sum(np.argmax(predictions_cln_fp_pd, axis=1) == np.argmax(y_test_cln_fp_pd, axis=1)) / len(y_test_cln_fp_pd)\n",
    "\n",
    "print(\"Accuracy on false positive benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_adv_tp_pd = classifier.predict(x_test_adv_tp_pd)\n",
    "accuracy = np.sum(np.argmax(predictions_adv_tp_pd, axis=1) == np.argmax(y_test_adv_tp_pd, axis=1)) / len(y_test_adv_tp_pd)\n",
    "\n",
    "print(\"Accuracy on true positive adversarial test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c0722",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_cln_fp_pd = classifier.predict(x_test_adv_fp_pd)\n",
    "accuracy = np.sum(np.argmax(predictions_adv_fp_pd, axis=1) == np.argmax(y_test_adv_fp_pd, axis=1)) / len(y_test_adv_fp_pd)\n",
    "\n",
    "print(\"Accuracy on false positive benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8236f2",
   "metadata": {},
   "source": [
    "Optional step: Plot all data pre- and post-transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot images\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "ax = plt.subplot(4, 1, 1)\n",
    "plt.imshow(x_test_cln_tp[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_cln_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 1, 2)\n",
    "plt.imshow(x_test_cln_tp_pd[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_cln_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 1)\n",
    "plt.imshow(x_test_cln_fp[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_cln_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 2, 2)\n",
    "plt.imshow(x_test_cln_fp_pd[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_cln_fp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 3, 1)\n",
    "plt.imshow(x_test_adv_tp[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_adv_tp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 3, 2)\n",
    "plt.imshow(x_test_adv_tp_pd[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_adv_tp_pd,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 4, 1)\n",
    "plt.imshow(x_test_adv_fp[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_adv_fp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "ax = plt.subplot(4, 4, 2)\n",
    "plt.imshow(x_test_adv_fp_pd[0], cmap='gray')\n",
    "ax.set_title('{:}'.format(np.argmax(y_test_adv_fp_dp,axis=1)[0]))\n",
    "plt.axis(\"off\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791c630",
   "metadata": {},
   "source": [
    "Optional step: Compare the performance of PixelDefend against the adversary over a range of eps values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_range = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "accuracy_original = []\n",
    "accuracy_robust = []\n",
    "\n",
    "adv_crafter = FastGradientMethod(classifier)\n",
    "adv_crafter_robust = FastGradientMethod(robust_classifier)\n",
    "\n",
    "for eps in eps_range:\n",
    "    adv_crafter.set_params(**{'eps': eps})\n",
    "    adv_crafter_robust.set_params(**{'eps': eps})\n",
    "    x_test_adv = adv_crafter.generate(x_test[:100])\n",
    "    x_test_adv_robust = adv_crafter_robust.generate(x_test[:100])\n",
    "    \n",
    "    predictions_original = np.argmax(classifier.predict(x_test_adv), axis=1)\n",
    "    accuracy_original += [np.sum(predictions_original == np.argmax(y_test[:100], axis=1))]\n",
    "    \n",
    "    predictions_robust = np.argmax(robust_classifier.predict(x_test_adv_robust), axis=1)\n",
    "    accuracy_robust += [np.sum(predictions_robust == np.argmax(y_test[:100], axis=1))]\n",
    "\n",
    "eps_range = eps_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8cbbd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.array(eps_range), np.array(accuracy_original), 'b--', label='Original classifier')\n",
    "ax.plot(np.array(eps_range), np.array(accuracy_robust), 'r--', label='Robust classifier')\n",
    "\n",
    "legend = ax.legend(loc='upper right', shadow=True, fontsize='large')\n",
    "#legend.get_frame().set_facecolor('#00FFCC')\n",
    "\n",
    "plt.xlabel('Attack strength (eps)')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e8b7ec",
   "metadata": {},
   "source": [
    "### **Load MARVEL dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = [] \n",
    "min_pixel_value = 0\n",
    "max_pixel_value = 1\n",
    "\n",
    "def marvel_class(filename):\n",
    "    switcher={\n",
    "        'HeavyLoadCarrier': 0,\n",
    "        'CombatVessel': 1,\n",
    "        'ContainerShip': 2,\n",
    "        'PassengersShip': 3,\n",
    "        'Ro-roCargo': 4,\n",
    "        'Tanker': 5,\n",
    "        'Tug': 6,\n",
    "        'SupplyVessel': 7,\n",
    "        'Yacht': 8 \n",
    "    }\n",
    "    return switcher.get(filename)\n",
    "\n",
    "def load_training_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/train_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_train.append(img/255)\n",
    "            y_train.append(marvel_class(filename))\n",
    "    return x_train, y_train\n",
    "\n",
    "def load_test_data(filename):\n",
    "    url = \"/home/cyber/Desktop/Adrian/marvel_data/test_9/\"+filename\n",
    "    for imgname in os.listdir(url):\n",
    "        img = cv2.imread(os.path.join(url,imgname))\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (320,240))\n",
    "            x_test.append(img/255)\n",
    "            y_test.append(marvel_class(filename))\n",
    "    return x_test, y_test\n",
    "\n",
    "# for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/train_9\"):\n",
    "#     load_training_data(filename)\n",
    "#     print(filename)\n",
    "\n",
    "for filename in os.listdir(\"/home/cyber/Desktop/Adrian/marvel_data/test_9\"):\n",
    "    load_test_data(filename)\n",
    "    print(filename)\n",
    "    \n",
    "#load_training_data(\"/home/cyber/Desktop/Adrian/marvel_data/test_9/CombatVessel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6df68",
   "metadata": {},
   "source": [
    "*Modification: Convert MARVEL x_test/x_train from uint8 into float32, to enable classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "133e4641",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn_classifier = TensorFlowV2Classifier(model=pixel_cnn, input_shape=input_shape, nb_classes=0)#, input_layer=1, output_layer=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b46512a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-f54b5305bbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_pixel_cnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-f54b5305bbe9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0;31m# training=True is only needed if there are layers with different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;31m# behavior during training versus inference (e.g. Dropout).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def train(model, data, optimizer):\n",
    "  for x, y in data:\n",
    "    with tf.GradientTape() as tape:\n",
    "      # training=True is only needed if there are layers with different\n",
    "      # behavior during training versus inference (e.g. Dropout).\n",
    "      prediction = model(x, training=True)\n",
    "      loss = loss_fn(prediction, y)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "train_pixel_cnn=train(pixel_cnn, data, 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e9912",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn_classifier = TensorFlowV2Classifier(model=pixel_cnn, nb_classes=10, train_step=train_pixel_cnn, input_shape=(28,28,1), clip_values=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f6c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_cnn_classifier.fit(x=x_train_cln, y=y_train_cln, batch_size=64, nb_epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
